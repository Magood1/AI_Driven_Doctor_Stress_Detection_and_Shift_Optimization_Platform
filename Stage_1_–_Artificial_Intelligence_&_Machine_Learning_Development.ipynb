{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "04086aadb0844d6794512523d5cf8576": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_53585a84148345db91a95575f8be81a1",
              "IPY_MODEL_3280eb4d24af4d70902c887466b29447",
              "IPY_MODEL_b272696999c849e1abecd9a37f36f283"
            ],
            "layout": "IPY_MODEL_89e3e2debaf642b8b4eba2e3de47c5d4"
          }
        },
        "53585a84148345db91a95575f8be81a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc5dc754f5bc4fe496d4dc7e7d19e6aa",
            "placeholder": "​",
            "style": "IPY_MODEL_66aaa68774284374bb15a14d60b6e8f9",
            "value": "model.safetensors: 100%"
          }
        },
        "3280eb4d24af4d70902c887466b29447": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9dfd3726a1f64a97a59186a2eb7caeca",
            "max": 21355344,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5b70efc3c8034697a1b0d3a45480b8df",
            "value": 21355344
          }
        },
        "b272696999c849e1abecd9a37f36f283": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c9294809cdb459fb3af6c9473042121",
            "placeholder": "​",
            "style": "IPY_MODEL_cc37e07eae324e7499ad80bb56b612c9",
            "value": " 21.4M/21.4M [00:00&lt;00:00, 48.8MB/s]"
          }
        },
        "89e3e2debaf642b8b4eba2e3de47c5d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc5dc754f5bc4fe496d4dc7e7d19e6aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66aaa68774284374bb15a14d60b6e8f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9dfd3726a1f64a97a59186a2eb7caeca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b70efc3c8034697a1b0d3a45480b8df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8c9294809cdb459fb3af6c9473042121": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc37e07eae324e7499ad80bb56b612c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"KAGGLE_USERNAME\"] = \"magdhndi\"\n",
        "os.environ[\"KAGGLE_KEY\"] = \"9b6c8953fb75d807a407f863ae22edc6\"\n",
        "\n",
        "# Download the datasets (replace with the actual Kaggle dataset URLs)\n",
        "!kaggle datasets download -d deadskull7/fer2013\n",
        "!kaggle datasets download -d jonathanoheix/affectnet\n",
        "!kaggle datasets download -d ahmedhamada/stress-faces\n",
        "# RAF-DB needs manual download as per the error message\n",
        "\n",
        "# Unzip the downloaded datasets\n",
        "!unzip fer2013.zip -d data/fer2013\n",
        "!unzip affectnet.zip -d data/affectnet\n",
        "!unzip stress-faces.zip -d data/stress-faces\n",
        "\n",
        "# Create the data directory if it doesn't exist\n",
        "!mkdir -p data/raf-db\n",
        "\n",
        "# Print confirmation messages\n",
        "print(\"Datasets downloaded and extracted successfully (except RAF-DB).\")\n",
        "print(\"Please manually download and place RAF-DB into the data/raf-db directory.\")\n",
        "\n",
        "# Verify if the datasets have been downloaded correctly (optional)\n",
        "import os\n",
        "datasets = [\"fer2013\", \"affectnet\", \"stress-faces\", \"raf-db\"]\n",
        "for dataset in datasets:\n",
        "    path = f\"data/{dataset}\"\n",
        "    if os.path.exists(path) and os.listdir(path):\n",
        "        print(f\"✅ {dataset} verification successful\")\n",
        "    elif dataset == \"raf-db\":\n",
        "        print(f\"⚠️ {dataset} requires manual download and placement.\")\n",
        "    else:\n",
        "        print(f\"❌ {dataset} verification failed. Directory not found or empty.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FO7WDhyS1s1g",
        "outputId": "7c8c2a82-d571-4ce8-97c5-50e5eaf5515f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/deadskull7/fer2013\n",
            "License(s): CC0-1.0\n",
            "fer2013.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "403 Client Error: Forbidden for url: https://www.kaggle.com/api/v1/datasets/metadata/jonathanoheix/affectnet\n",
            "403 Client Error: Forbidden for url: https://www.kaggle.com/api/v1/datasets/metadata/ahmedhamada/stress-faces\n",
            "Archive:  fer2013.zip\n",
            "  inflating: data/fer2013/fer2013.csv  \n",
            "unzip:  cannot find or open affectnet.zip, affectnet.zip.zip or affectnet.zip.ZIP.\n",
            "unzip:  cannot find or open stress-faces.zip, stress-faces.zip.zip or stress-faces.zip.ZIP.\n",
            "Datasets downloaded and extracted successfully (except RAF-DB).\n",
            "Please manually download and place RAF-DB into the data/raf-db directory.\n",
            "✅ fer2013 verification successful\n",
            "❌ affectnet verification failed. Directory not found or empty.\n",
            "❌ stress-faces verification failed. Directory not found or empty.\n",
            "⚠️ raf-db requires manual download and placement.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NN3gOSxbMseq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c96b894-aa18-41d2-d8f7-dcc172eddb4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Verifying Datasets ---\n",
            "\n",
            "Verifying fer2013...\n",
            "✔️ Directory Found: data/fer2013\n",
            "    ✔️ Found 1 match(es) for: 'fer2013.csv'\n",
            "\n",
            "Verifying affectnet...\n",
            "❌ Verification Failed: Directory not found - data/affectnet\n",
            "\n",
            "Verifying stress-faces...\n",
            "❌ Verification Failed: Directory not found - data/stress-faces\n",
            "\n",
            "Verifying raf-db...\n",
            "✔️ Directory Found: data/raf-db\n",
            "    ❌ Expected file/pattern not found: 'basic/Image/original/*.jpg'\n",
            "    ❌ Expected file/pattern not found: 'basic/EmoLabel/list_patition_label.txt'\n",
            "   -> Note: RAF-DB requires manual download and placement.\n",
            "\n",
            "⚠️ Some datasets are missing or incomplete. Please check downloads/paths.\n"
          ]
        }
      ],
      "source": [
        "# @title <<< Stage 1 / Task 1.1: Dataset Verification >>>\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "os.environ[\"KAGGLE_USERNAME\"] = \"magdhndi\"\n",
        "os.environ[\"KAGGLE_KEY\"] = \"9b6c8953fb75d807a407f863ae22edc6\"\n",
        "\n",
        "# Directories defined previously\n",
        "DATA_DIR = Path(\"./data\")\n",
        "FER2013_DIR = DATA_DIR / \"fer2013\"\n",
        "AFFECTNET_DIR = DATA_DIR / \"affectnet\"\n",
        "RAFDB_DIR = DATA_DIR / \"raf-db\" # Assumed manual download\n",
        "STRESS_FACES_DIR = DATA_DIR / \"stress-faces\"\n",
        "\n",
        "def verify_dataset(dir_path, expected_files_or_patterns):\n",
        "    \"\"\"Checks if a directory exists and contains expected files/patterns.\"\"\"\n",
        "    if not dir_path.exists() or not dir_path.is_dir():\n",
        "        print(f\"❌ Verification Failed: Directory not found - {dir_path}\")\n",
        "        return False\n",
        "    print(f\"✔️ Directory Found: {dir_path}\")\n",
        "\n",
        "    found_all = True\n",
        "    for pattern in expected_files_or_patterns:\n",
        "        matches = list(dir_path.glob(pattern))\n",
        "        if not matches:\n",
        "            print(f\"    ❌ Expected file/pattern not found: '{pattern}'\")\n",
        "            found_all = False\n",
        "        else:\n",
        "             print(f\"    ✔️ Found {len(matches)} match(es) for: '{pattern}'\")\n",
        "    return found_all\n",
        "\n",
        "print(\"--- Verifying Datasets ---\")\n",
        "verification_checks = {\n",
        "    FER2013_DIR: [\"fer2013.csv\"],\n",
        "    AFFECTNET_DIR: [\"training.csv\", \"images/*.jpg\"], # Check for CSV and some images\n",
        "    STRESS_FACES_DIR: [\"Level_*/*.png\"], # Check for level dirs and png images\n",
        "    RAFDB_DIR: [\"basic/Image/original/*.jpg\", \"basic/EmoLabel/list_patition_label.txt\"] # Check structure if manually downloaded\n",
        "}\n",
        "\n",
        "all_verified = True\n",
        "for dir_path, patterns in verification_checks.items():\n",
        "    print(f\"\\nVerifying {dir_path.name}...\")\n",
        "    if not verify_dataset(dir_path, patterns):\n",
        "        all_verified = False\n",
        "        if dir_path == RAFDB_DIR:\n",
        "            print(\"   -> Note: RAF-DB requires manual download and placement.\")\n",
        "\n",
        "if all_verified:\n",
        "    print(\"\\n✅ All specified datasets seem to be present.\")\n",
        "else:\n",
        "    print(\"\\n⚠️ Some datasets are missing or incomplete. Please check downloads/paths.\")\n",
        "\n",
        "# Further loading logic depends on which datasets are successfully verified and needed.\n",
        "# We'll continue using Stress Faces as the primary example for simplicity."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install mediapipe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FQnbpXf2LJO",
        "outputId": "b73a4a8a-2b01-4d43-896a-96d07fde00e1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.21-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.2.10)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.2)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (3.10.0)\n",
            "Collecting numpy<2 (from mediapipe)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.11.0.86)\n",
            "Collecting protobuf<5,>=4.25.3 (from mediapipe)\n",
            "  Downloading protobuf-4.25.7-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (1.15.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
            "Downloading mediapipe-0.10.21-cp311-cp311-manylinux_2_28_x86_64.whl (35.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.7-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.1-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: protobuf, numpy, sounddevice, mediapipe\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.4\n",
            "    Uninstalling protobuf-5.29.4:\n",
            "      Successfully uninstalled protobuf-5.29.4\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.7 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.7 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed mediapipe-0.10.21 numpy-1.26.4 protobuf-4.25.7 sounddevice-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade --force-reinstall mediapipe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vfgB8VKf2uuY",
        "outputId": "b5efe7e6-c7a8-4fb6-ae63-5e9e9d189789"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mediapipe\n",
            "  Using cached mediapipe-0.10.21-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Collecting absl-py (from mediapipe)\n",
            "  Downloading absl_py-2.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting attrs>=19.1.0 (from mediapipe)\n",
            "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting flatbuffers>=2.0 (from mediapipe)\n",
            "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Collecting jax (from mediapipe)\n",
            "  Downloading jax-0.6.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib (from mediapipe)\n",
            "  Downloading jaxlib-0.6.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting matplotlib (from mediapipe)\n",
            "  Downloading matplotlib-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting numpy<2 (from mediapipe)\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting opencv-contrib-python (from mediapipe)\n",
            "  Downloading opencv_contrib_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting protobuf<5,>=4.25.3 (from mediapipe)\n",
            "  Using cached protobuf-4.25.7-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Using cached sounddevice-0.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting sentencepiece (from mediapipe)\n",
            "  Downloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting CFFI>=1.0 (from sounddevice>=0.4.4->mediapipe)\n",
            "  Downloading cffi-1.17.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting ml_dtypes>=0.5.0 (from jax->mediapipe)\n",
            "  Downloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Collecting opt_einsum (from jax->mediapipe)\n",
            "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting scipy>=1.11.1 (from jax->mediapipe)\n",
            "  Downloading scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting contourpy>=1.0.1 (from matplotlib->mediapipe)\n",
            "  Downloading contourpy-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib->mediapipe)\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib->mediapipe)\n",
            "  Downloading fonttools-4.57.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (102 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.5/102.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib->mediapipe)\n",
            "  Downloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
            "Collecting packaging>=20.0 (from matplotlib->mediapipe)\n",
            "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting pillow>=8 (from matplotlib->mediapipe)\n",
            "  Downloading pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
            "Collecting pyparsing>=2.3.1 (from matplotlib->mediapipe)\n",
            "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting python-dateutil>=2.7 (from matplotlib->mediapipe)\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pycparser (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe)\n",
            "  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
            "Collecting six>=1.5 (from python-dateutil>=2.7->matplotlib->mediapipe)\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Using cached mediapipe-0.10.21-cp311-cp311-manylinux_2_28_x86_64.whl (35.6 MB)\n",
            "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Using cached protobuf-4.25.7-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "Using cached sounddevice-0.5.1-py3-none-any.whl (32 kB)\n",
            "Downloading absl_py-2.2.2-py3-none-any.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax-0.6.0-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxlib-0.6.0-cp311-cp311-manylinux2014_x86_64.whl (87.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.8/87.8 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m116.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_contrib_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (69.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cffi-1.17.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (467 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.2/467.2 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contourpy-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.2/326.2 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading fonttools-4.57.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m104.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-25.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m101.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.1/111.1 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.6/117.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece, flatbuffers, six, pyparsing, pycparser, protobuf, pillow, packaging, opt_einsum, numpy, kiwisolver, fonttools, cycler, attrs, absl-py, scipy, python-dateutil, opencv-contrib-python, ml_dtypes, contourpy, CFFI, sounddevice, matplotlib, jaxlib, jax, mediapipe\n",
            "  Attempting uninstall: sentencepiece\n",
            "    Found existing installation: sentencepiece 0.2.0\n",
            "    Uninstalling sentencepiece-0.2.0:\n",
            "      Successfully uninstalled sentencepiece-0.2.0\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 25.2.10\n",
            "    Uninstalling flatbuffers-25.2.10:\n",
            "      Successfully uninstalled flatbuffers-25.2.10\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.17.0\n",
            "    Uninstalling six-1.17.0:\n",
            "      Successfully uninstalled six-1.17.0\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.2.3\n",
            "    Uninstalling pyparsing-3.2.3:\n",
            "      Successfully uninstalled pyparsing-3.2.3\n",
            "  Attempting uninstall: pycparser\n",
            "    Found existing installation: pycparser 2.22\n",
            "    Uninstalling pycparser-2.22:\n",
            "      Successfully uninstalled pycparser-2.22\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.7\n",
            "    Uninstalling protobuf-4.25.7:\n",
            "      Successfully uninstalled protobuf-4.25.7\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.2.1\n",
            "    Uninstalling pillow-11.2.1:\n",
            "      Successfully uninstalled pillow-11.2.1\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.2\n",
            "    Uninstalling packaging-24.2:\n",
            "      Successfully uninstalled packaging-24.2\n",
            "  Attempting uninstall: opt_einsum\n",
            "    Found existing installation: opt_einsum 3.4.0\n",
            "    Uninstalling opt_einsum-3.4.0:\n",
            "      Successfully uninstalled opt_einsum-3.4.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: kiwisolver\n",
            "    Found existing installation: kiwisolver 1.4.8\n",
            "    Uninstalling kiwisolver-1.4.8:\n",
            "      Successfully uninstalled kiwisolver-1.4.8\n",
            "  Attempting uninstall: fonttools\n",
            "    Found existing installation: fonttools 4.57.0\n",
            "    Uninstalling fonttools-4.57.0:\n",
            "      Successfully uninstalled fonttools-4.57.0\n",
            "  Attempting uninstall: cycler\n",
            "    Found existing installation: cycler 0.12.1\n",
            "    Uninstalling cycler-0.12.1:\n",
            "      Successfully uninstalled cycler-0.12.1\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 25.3.0\n",
            "    Uninstalling attrs-25.3.0:\n",
            "      Successfully uninstalled attrs-25.3.0\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 1.4.0\n",
            "    Uninstalling absl-py-1.4.0:\n",
            "      Successfully uninstalled absl-py-1.4.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.2\n",
            "    Uninstalling scipy-1.15.2:\n",
            "      Successfully uninstalled scipy-1.15.2\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.9.0.post0\n",
            "    Uninstalling python-dateutil-2.9.0.post0:\n",
            "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
            "  Attempting uninstall: opencv-contrib-python\n",
            "    Found existing installation: opencv-contrib-python 4.11.0.86\n",
            "    Uninstalling opencv-contrib-python-4.11.0.86:\n",
            "      Successfully uninstalled opencv-contrib-python-4.11.0.86\n",
            "  Attempting uninstall: ml_dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: contourpy\n",
            "    Found existing installation: contourpy 1.3.2\n",
            "    Uninstalling contourpy-1.3.2:\n",
            "      Successfully uninstalled contourpy-1.3.2\n",
            "  Attempting uninstall: CFFI\n",
            "    Found existing installation: cffi 1.17.1\n",
            "    Uninstalling cffi-1.17.1:\n",
            "      Successfully uninstalled cffi-1.17.1\n",
            "  Attempting uninstall: sounddevice\n",
            "    Found existing installation: sounddevice 0.5.1\n",
            "    Uninstalling sounddevice-0.5.1:\n",
            "      Successfully uninstalled sounddevice-0.5.1\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.0\n",
            "    Uninstalling matplotlib-3.10.0:\n",
            "      Successfully uninstalled matplotlib-3.10.0\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.5.1\n",
            "    Uninstalling jaxlib-0.5.1:\n",
            "      Successfully uninstalled jaxlib-0.5.1\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.5.2\n",
            "    Uninstalling jax-0.5.2:\n",
            "      Successfully uninstalled jax-0.5.2\n",
            "  Attempting uninstall: mediapipe\n",
            "    Found existing installation: mediapipe 0.10.21\n",
            "    Uninstalling mediapipe-0.10.21:\n",
            "      Successfully uninstalled mediapipe-0.10.21\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.7 which is incompatible.\n",
            "tensorflow 2.18.0 requires ml-dtypes<0.5.0,>=0.4.0, but you have ml-dtypes 0.5.1 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.7 which is incompatible.\n",
            "langchain-core 0.3.56 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed CFFI-1.17.1 absl-py-2.2.2 attrs-25.3.0 contourpy-1.3.2 cycler-0.12.1 flatbuffers-25.2.10 fonttools-4.57.0 jax-0.6.0 jaxlib-0.6.0 kiwisolver-1.4.8 matplotlib-3.10.1 mediapipe-0.10.21 ml_dtypes-0.5.1 numpy-1.26.4 opencv-contrib-python-4.11.0.86 opt_einsum-3.4.0 packaging-25.0 pillow-11.2.1 protobuf-4.25.7 pycparser-2.22 pyparsing-3.2.3 python-dateutil-2.9.0.post0 scipy-1.15.2 sentencepiece-0.2.0 six-1.17.0 sounddevice-0.5.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "_cffi_backend",
                  "_sounddevice",
                  "absl",
                  "cv2",
                  "cycler",
                  "dateutil",
                  "flatbuffers",
                  "jax",
                  "jaxlib",
                  "kiwisolver",
                  "mediapipe",
                  "ml_dtypes",
                  "six"
                ]
              },
              "id": "e852d4f8240c4b32ad90bae5c80462bd"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "source": [
        "# @title <<< Stage 1 / Task 1.2: Face Detection & Landmarks (MediaPipe Setup) >>>\n",
        "\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Ensure MediaPipe objects are initialized (code from previous step)\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "\n",
        "# Explicitly specify the model path if necessary\n",
        "# For example:\n",
        "# model_path = \"path/to/your/face_landmark.tflite\"\n",
        "# face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, refine_landmarks=True, min_detection_confidence=0.5, model_path=model_path)\n",
        "\n",
        "# If you are using the default model, try reinstalling mediapipe:\n",
        "# !pip install --upgrade --force-reinstall mediapipe\n",
        "\n",
        "face_mesh = mp_face_mesh.FaceMesh(\n",
        "    static_image_mode=True,\n",
        "    max_num_faces=1,\n",
        "    refine_landmarks=True, # Use 478 landmarks\n",
        "    min_detection_confidence=0.5)\n",
        "\n",
        "# Re-define or import the detect_face_and_landmarks_mediapipe function here\n",
        "# (Assuming it's available from the previous execution cell)\n",
        "# [Insert the detect_face_and_landmarks_mediapipe function code here if running standalone]\n",
        "\n",
        "print(\"✔️ MediaPipe FaceMesh (478 landmarks) initialized.\")\n",
        "# Example Usage (Optional):\n",
        "# img_path_example = list(STRESS_FACES_DIR.glob('Level_0/*.png'))[0]\n",
        "# img_example = cv2.imread(str(img_path_example))\n",
        "# landmarks, bbox, face_crop = detect_face_and_landmarks_mediapipe(img_example)\n",
        "# if landmarks is not None:\n",
        "#   print(f\"Detected face and {landmarks.shape[0]} landmarks.\")\n",
        "# else:\n",
        "#   print(\"No face detected in example image.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPjwOZnq2mzD",
        "outputId": "e33b6f57-fa47-432a-84e7-e9493000a1f0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/jaxlib/plugin_support.py:71: RuntimeWarning: JAX plugin jax_cuda12_plugin version 0.5.1 is installed, but it is not compatible with the installed jaxlib version 0.6.0, so it will not be used.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔️ MediaPipe FaceMesh (478 landmarks) initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title <<< Stage 1 / Task 1.3: rPPG Estimation (CHROM Implementation) >>>\n",
        "\n",
        "import numpy as np\n",
        "import scipy.signal\n",
        "from scipy.signal import butter, filtfilt\n",
        "from scipy.fft import fft, fftfreq\n",
        "\n",
        "# --- Constants for rPPG ---\n",
        "# Typical physiological heart rate range in Hz\n",
        "MIN_HR_HZ = 0.75 # 45 bpm\n",
        "MAX_HR_HZ = 3.0  # 180 bpm\n",
        "# ROI selection using MediaPipe landmarks (approximate forehead/cheeks)\n",
        "# These indices might need adjustment based on the specific MediaPipe model version (468 vs 478)\n",
        "# Example indices for cheeks (adjust as needed):\n",
        "LEFT_CHEEK_IDXS = [234, 127, 162, 21, 54, 103, 67, 109] # Example set\n",
        "RIGHT_CHEEK_IDXS = [454, 356, 389, 251, 284, 332, 297, 338] # Example set\n",
        "FOREHEAD_IDXS = [10, 338, 297, 332, 284, 251, 389, 356, 454, 323, 447, 266, 229, 228, 29] # Example\n",
        "\n",
        "def select_rois(landmarks_sequence, frame_shape, roi_indices):\n",
        "    \"\"\"Selects pixel coordinates within ROIs based on landmarks.\"\"\"\n",
        "    # landmarks_sequence: list/array of landmark arrays [(N_lm, 3), ...]\n",
        "    # frame_shape: (height, width)\n",
        "    # roi_indices: list of landmark indices defining the ROI polygon(s)\n",
        "\n",
        "    # This is simplified: For robustness, average coordinates within polygons\n",
        "    # defined by roi_indices for each frame. Here, we just take the mean\n",
        "    # location of the indices for simplicity of demonstration.\n",
        "    # A better approach involves creating masks.\n",
        "\n",
        "    h, w = frame_shape\n",
        "    roi_pixels_sequence = []\n",
        "\n",
        "    for landmarks in landmarks_sequence:\n",
        "        if landmarks is None:\n",
        "            roi_pixels_sequence.append(None) # Handle frames where landmarks failed\n",
        "            continue\n",
        "\n",
        "        # Get coordinates for ROI indices, scale to pixel values\n",
        "        roi_coords = landmarks[roi_indices, :2] * [w, h]\n",
        "        roi_coords = roi_coords.astype(int)\n",
        "\n",
        "        # Basic mean value within the bounding box of ROI points (simplistic)\n",
        "        # TODO: Implement proper masking for better ROI analysis\n",
        "        x_min, y_min = roi_coords.min(axis=0)\n",
        "        x_max, y_max = roi_coords.max(axis=0)\n",
        "        # Ensure bounds are valid\n",
        "        x_min, y_min = max(0, x_min), max(0, y_min)\n",
        "        x_max, y_max = min(w - 1, x_max), min(h - 1, y_max)\n",
        "\n",
        "        if x_max > x_min and y_max > y_min:\n",
        "             roi_pixels_sequence.append({'x_min': x_min, 'y_min': y_min, 'x_max': x_max, 'y_max': y_max})\n",
        "        else:\n",
        "             roi_pixels_sequence.append(None)\n",
        "\n",
        "    return roi_pixels_sequence\n",
        "\n",
        "\n",
        "def extract_mean_rgb(frames_sequence, rois_sequence):\n",
        "    \"\"\"Extracts mean RGB values from ROIs over a sequence of frames.\"\"\"\n",
        "    mean_rgb_signal = []\n",
        "    if len(frames_sequence) != len(rois_sequence):\n",
        "         raise ValueError(\"Frames and ROIs sequence lengths mismatch.\")\n",
        "\n",
        "    for frame, roi_bbox in zip(frames_sequence, rois_sequence):\n",
        "        if frame is None or roi_bbox is None:\n",
        "            # Handle missing data, e.g., append NaN or previous value\n",
        "            mean_rgb_signal.append([np.nan, np.nan, np.nan])\n",
        "            continue\n",
        "\n",
        "        x_min, y_min = roi_bbox['x_min'], roi_bbox['y_min']\n",
        "        x_max, y_max = roi_bbox['x_max'], roi_bbox['y_max']\n",
        "\n",
        "        roi_patch = frame[y_min:y_max, x_min:x_max]\n",
        "        if roi_patch.size == 0:\n",
        "            mean_rgb_signal.append([np.nan, np.nan, np.nan])\n",
        "            continue\n",
        "\n",
        "        # Calculate mean R, G, B in the ROI\n",
        "        mean_rgb = np.mean(roi_patch, axis=(0, 1))\n",
        "        mean_rgb_signal.append(mean_rgb)\n",
        "\n",
        "    return np.array(mean_rgb_signal) # Shape (num_frames, 3)\n",
        "\n",
        "\n",
        "def bandpass_filter(signal, lowcut, highcut, fs, order=5):\n",
        "    \"\"\"Applies a Butterworth bandpass filter.\"\"\"\n",
        "    nyquist = 0.5 * fs\n",
        "    low = lowcut / nyquist\n",
        "    high = highcut / nyquist\n",
        "    # Handle potential edge cases for low/high frequencies\n",
        "    low = max(low, 1e-6)  # Prevent division by zero or negative frequencies\n",
        "    high = min(high, 1.0 - 1e-6) # Ensure high is less than 1.0\n",
        "\n",
        "    if low >= high:\n",
        "        print(f\"Warning: Low cut ({lowcut} Hz) >= High cut ({highcut} Hz). Skipping filter.\")\n",
        "        return signal # Return unfiltered signal or handle as error\n",
        "\n",
        "    try:\n",
        "      b, a = butter(order, [low, high], btype='band')\n",
        "      filtered_signal = filtfilt(b, a, signal, axis=0)\n",
        "      return filtered_signal\n",
        "    except ValueError as e:\n",
        "       print(f\"Error during filtering: {e}. Signal length might be too short for filter order.\")\n",
        "       # Return original signal or NaN array of same shape\n",
        "       return signal # Or np.full_like(signal, np.nan)\n",
        "\n",
        "\n",
        "def estimate_rppg_chrom(rgb_signal, fs):\n",
        "    \"\"\"\n",
        "    Estimates heart rate from mean RGB signal using the CHROM method.\n",
        "\n",
        "    Args:\n",
        "        rgb_signal (np.ndarray): Array of mean RGB values per frame, shape (num_frames, 3).\n",
        "        fs (float): Sampling frequency (frames per second).\n",
        "\n",
        "    Returns:\n",
        "        float: Estimated heart rate in BPM, or None if estimation fails.\n",
        "    \"\"\"\n",
        "    if rgb_signal is None or len(rgb_signal) < int(fs * 2): # Need at least a few seconds\n",
        "        print(\"Warning: Insufficient signal length for rPPG.\")\n",
        "        return None\n",
        "    # Handle NaNs if any (e.g., simple interpolation or skip)\n",
        "    if np.isnan(rgb_signal).any():\n",
        "        print(\"Warning: NaN values detected in RGB signal, attempting interpolation.\")\n",
        "        # Simple linear interpolation - more robust methods might be needed\n",
        "        for i in range(3): # Interpolate each channel R, G, B\n",
        "             channel = rgb_signal[:, i]\n",
        "             nan_indices = np.isnan(channel)\n",
        "             if np.all(nan_indices): return None # Cannot interpolate if all are NaN\n",
        "             indices = lambda z: z.nonzero()[0]\n",
        "             channel[nan_indices] = np.interp(indices(nan_indices), indices(~nan_indices), channel[~nan_indices])\n",
        "        if np.isnan(rgb_signal).any(): return None # Failed interpolation\n",
        "\n",
        "    # 1. Bandpass filter RGB signals\n",
        "    filtered_rgb = bandpass_filter(rgb_signal, MIN_HR_HZ, MAX_HR_HZ, fs)\n",
        "    if filtered_rgb is None: return None\n",
        "\n",
        "    # 2. Calculate CHROM signal (Simplified version)\n",
        "    # Xs = 3*Rn - 2*Gn (approximation of projection orthogonal to specular)\n",
        "    # Ys = 1.5*Rn + Gn - 1.5*Bn (approximation)\n",
        "    # More standard: Use standardized signals\n",
        "    mean_rgb = np.mean(filtered_rgb, axis=0)\n",
        "    std_rgb = np.std(filtered_rgb, axis=0)\n",
        "    if np.any(std_rgb == 0): return None # Avoid division by zero\n",
        "    normalized_rgb = (filtered_rgb - mean_rgb) / std_rgb\n",
        "\n",
        "    Rn, Gn, Bn = normalized_rgb[:, 0], normalized_rgb[:, 1], normalized_rgb[:, 2]\n",
        "    Xs = 3 * Rn - 2 * Gn\n",
        "    Ys = 1.5 * Rn + Gn - 1.5 * Bn\n",
        "\n",
        "    # Alpha calculation (ratio of std deviations)\n",
        "    alpha = np.std(Xs) / (np.std(Ys) + 1e-6) # Add epsilon for stability\n",
        "    chrom_signal = Xs - alpha * Ys\n",
        "\n",
        "    # 3. FFT to find dominant frequency\n",
        "    N = len(chrom_signal)\n",
        "    if N == 0: return None\n",
        "\n",
        "    yf = fft(chrom_signal)\n",
        "    xf = fftfreq(N, 1 / fs) # Frequency bins\n",
        "\n",
        "    # Consider only positive frequencies within the valid HR range\n",
        "    freq_mask = (xf >= MIN_HR_HZ) & (xf <= MAX_HR_HZ)\n",
        "    if not np.any(freq_mask):\n",
        "         print(\"Warning: No frequency components found in the valid HR range.\")\n",
        "         return None\n",
        "\n",
        "    fft_power = np.abs(yf[freq_mask])**2\n",
        "    valid_freqs = xf[freq_mask]\n",
        "\n",
        "    if len(valid_freqs) == 0:\n",
        "         print(\"Warning: No valid frequencies after masking.\")\n",
        "         return None\n",
        "\n",
        "    # Find the frequency with the maximum power\n",
        "    peak_freq_index = np.argmax(fft_power)\n",
        "    dominant_freq = valid_freqs[peak_freq_index]\n",
        "\n",
        "    # 4. Convert frequency to BPM\n",
        "    estimated_hr_bpm = dominant_freq * 60\n",
        "\n",
        "    return estimated_hr_bpm\n",
        "\n",
        "# --- Design Note ---\n",
        "# This rPPG implementation is basic. Production systems often use:\n",
        "# - More robust ROI tracking (e.g., Kalman filters).\n",
        "# - Advanced filtering and detrending (e.g., wavelet transforms, empirical mode decomposition).\n",
        "# - Different methods (POS, ICA-based) that might be more robust to motion/lighting.\n",
        "# - Windowing techniques (calculating HR over sliding windows).\n",
        "# - Quality assessment metrics for the PPG signal.\n",
        "# It *requires* a sequence of frames (e.g., 5-10 seconds at ~30fps).\n",
        "\n",
        "print(\"✔️ Basic rPPG (CHROM) estimation functions defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hz1eDnkA2W71",
        "outputId": "b7bd401c-0635-4248-dae6-1e99fafa4754"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔️ Basic rPPG (CHROM) estimation functions defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title <<< Stage 1 / Task 1.4: Data Augmentation (Albumentations Setup) >>>\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import cv2\n",
        "\n",
        "# Re-define transforms if needed (assuming available from previous step)\n",
        "# [Insert train_transform and val_test_transform definitions here if running standalone]\n",
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH = 224\n",
        "\n",
        "train_transform = A.Compose([\n",
        "    A.Resize(height=IMG_HEIGHT, width=IMG_WIDTH, interpolation=cv2.INTER_LINEAR),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.Rotate(limit=15, p=0.3, border_mode=cv2.BORDER_CONSTANT),\n",
        "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
        "    A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.3),\n",
        "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.2),\n",
        "    A.MotionBlur(blur_limit=7, p=0.2),\n",
        "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # ImageNet normalization\n",
        "    ToTensorV2() # Convert to PyTorch tensor C, H, W\n",
        "])\n",
        "\n",
        "val_test_transform = A.Compose([\n",
        "    A.Resize(height=IMG_HEIGHT, width=IMG_WIDTH, interpolation=cv2.INTER_LINEAR),\n",
        "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "\n",
        "print(\"✔️ Albumentations transforms (train/val/test) defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvXopSnz3MiS",
        "outputId": "de27505d-bdd8-4f5c-b4ff-fbb278674f41"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔️ Albumentations transforms (train/val/test) defined.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-fd5f5a1071e5>:18: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
            "  A.GaussNoise(var_limit=(10.0, 50.0), p=0.2),\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title <<< Stage 1 / Task 1.5: Input Validation Models (Pydantic) >>>\n",
        "\n",
        "from pydantic import BaseModel, Field, validator\n",
        "from typing import List, Optional\n",
        "import base64\n",
        "\n",
        "class FrameInput(BaseModel):\n",
        "    \"\"\"Input model for a single frame.\"\"\"\n",
        "    image_b64: str # Base64 encoded image string\n",
        "    timestamp: Optional[float] = None # Optional timestamp in seconds\n",
        "\n",
        "    @validator('image_b64')\n",
        "    def check_base64(cls, v):\n",
        "        try:\n",
        "            base64.b64decode(v, validate=True)\n",
        "            return v\n",
        "        except (TypeError, ValueError, base64.binascii.Error):\n",
        "            raise ValueError(\"Invalid base64 string\")\n",
        "\n",
        "class VideoInput(BaseModel):\n",
        "    \"\"\"Input model for sequence of frames (video clip).\"\"\"\n",
        "    frames: List[FrameInput]\n",
        "    fps: Optional[float] = Field(None, gt=0) # Frames per second, must be positive\n",
        "    doctor_id: Optional[str] = None # Identifier for context\n",
        "    # Add other metadata if needed\n",
        "\n",
        "class FeatureInput(BaseModel):\n",
        "    \"\"\"Input model if features are pre-extracted.\"\"\"\n",
        "    feature_vector: List[float]\n",
        "    sequence_id: Optional[str] = None\n",
        "    doctor_id: Optional[str] = None\n",
        "\n",
        "print(\"✔️ Pydantic models for API input validation defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oBvqnYo3TOC",
        "outputId": "749c6b2a-6898-4917-cb29-f9808321ede5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔️ Pydantic models for API input validation defined.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-ab89c15b2f12>:12: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  @validator('image_b64')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title <<< Stage 1 / Task 2.1: Model Architectures (PyTorch) >>>\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import timm # PyTorch Image Models library - very useful!\n",
        "\n",
        "# --- Model 1: EfficientNet-B0 ---\n",
        "def build_efficientnet_b0(num_classes, pretrained=True):\n",
        "    \"\"\"Builds an EfficientNet-B0 model for classification.\"\"\"\n",
        "    # Use timm for easy access to pretrained models\n",
        "    model = timm.create_model('efficientnet_b0', pretrained=pretrained)\n",
        "\n",
        "    # Replace the classifier layer\n",
        "    num_ftrs = model.classifier.in_features\n",
        "    model.classifier = nn.Sequential(\n",
        "        nn.Linear(num_ftrs, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(512, num_classes)\n",
        "        # Use nn.Sigmoid() here if doing binary classification with BCEWithLogitsLoss,\n",
        "        # or keep as is for CrossEntropyLoss (expects raw logits)\n",
        "    )\n",
        "    print(f\"Built EfficientNet-B0 with {num_classes} output classes.\")\n",
        "    return model\n",
        "\n",
        "# --- Model 2: ResNet50 + GRU (Outline) ---\n",
        "class ResNetGRUModel(nn.Module):\n",
        "    def __init__(self, num_classes, gru_hidden_size=256, gru_layers=1, pretrained=True, dropout=0.5):\n",
        "        super().__init__()\n",
        "        # Load pretrained ResNet50, remove classifier\n",
        "        resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1 if pretrained else None)\n",
        "        self.features = nn.Sequential(*list(resnet.children())[:-1]) # Remove final FC layer\n",
        "        resnet_out_features = resnet.fc.in_features # Typically 2048\n",
        "\n",
        "        # GRU layer\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=resnet_out_features,\n",
        "            hidden_size=gru_hidden_size,\n",
        "            num_layers=gru_layers,\n",
        "            batch_first=True, # Expect input shape (batch, seq_len, features)\n",
        "            bidirectional=False, # Or True if context from both directions helps\n",
        "            dropout=dropout if gru_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        # Classifier head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(gru_hidden_size, num_classes) # Adjust if bidirectional\n",
        "        )\n",
        "        print(f\"Built ResNet50+GRU with {num_classes} output classes.\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch, seq_len, channels, height, width)\n",
        "        b, s, c, h, w = x.shape\n",
        "        x = x.view(b * s, c, h, w) # Combine batch and sequence dims for CNN\n",
        "\n",
        "        # Pass through ResNet feature extractor\n",
        "        x = self.features(x) # Shape: (b*s, resnet_out_features, 1, 1)\n",
        "        x = x.view(b, s, -1) # Reshape back to (b, s, resnet_out_features)\n",
        "\n",
        "        # Pass through GRU\n",
        "        # self.gru.flatten_parameters() # Optional: Use if using DataParallel\n",
        "        out, _ = self.gru(x) # out shape: (b, s, gru_hidden_size)\n",
        "\n",
        "        # Use the output of the last time step for classification\n",
        "        last_step_out = out[:, -1, :] # Shape: (b, gru_hidden_size)\n",
        "\n",
        "        # Pass through classifier\n",
        "        logits = self.classifier(last_step_out)\n",
        "        return logits\n",
        "\n",
        "# --- Model 3: MobileNetV3 + LSTM (Outline) ---\n",
        "class MobileNetLSTMModel(nn.Module):\n",
        "    def __init__(self, num_classes, lstm_hidden_size=256, lstm_layers=1, pretrained=True, dropout=0.5):\n",
        "        super().__init__()\n",
        "        # Load pretrained MobileNetV3 Large/Small\n",
        "        mobilenet = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.IMAGENET1K_V1 if pretrained else None)\n",
        "        # Use features part, remove classifier\n",
        "        self.features = mobilenet.features\n",
        "        # Determine output features of the feature extractor\n",
        "        # For MobileNetV3 Large, the avgpool output before classifier is 960\n",
        "        # Find this programmatically or hardcode based on architecture knowledge\n",
        "        dummy_input = torch.randn(1, 3, 224, 224)\n",
        "        dummy_out = self.features(dummy_input)\n",
        "        # The output shape might be (1, channels, H', W') - need global avg pooling\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        mobilenet_out_features = self.features[-1][0].out_channels # Heuristic: channels of last conv block\n",
        "        # Need to verify mobilenet_out_features carefully - print model structure or test output shape\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=mobilenet_out_features, # Adjust this value based on actual output\n",
        "            hidden_size=lstm_hidden_size,\n",
        "            num_layers=lstm_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=False,\n",
        "            dropout=dropout if lstm_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        # Classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(lstm_hidden_size, num_classes) # Adjust if bidirectional\n",
        "        )\n",
        "        print(f\"Built MobileNetV3+LSTM with {num_classes} output classes.\")\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch, seq_len, channels, height, width)\n",
        "        b, s, c, h, w = x.shape\n",
        "        x = x.view(b * s, c, h, w)\n",
        "\n",
        "        x = self.features(x) # Shape: (b*s, channels, H', W')\n",
        "        x = self.avgpool(x) # Shape: (b*s, channels, 1, 1)\n",
        "        x = x.view(b, s, -1) # Reshape to (b, s, mobilenet_out_features)\n",
        "\n",
        "        # self.lstm.flatten_parameters()\n",
        "        out, _ = self.lstm(x) # out shape: (b, s, lstm_hidden_size)\n",
        "        last_step_out = out[:, -1, :] # Shape: (b, lstm_hidden_size)\n",
        "\n",
        "        logits = self.classifier(last_step_out)\n",
        "        return logits\n",
        "\n",
        "\n",
        "# --- Design Note on Sequence Models ---\n",
        "# - Training requires DataLoaders that yield sequences (clips) of frames.\n",
        "# - This involves grouping frames from videos or creating sequences from datasets like AffectNet if timestamps are available.\n",
        "# - The `FaceStressDataset` needs modification to handle video clips or sequences.\n",
        "# - For now, we proceed with EfficientNet (image-based) for simplicity in training setup.\n",
        "\n",
        "# Example: Instantiate EfficientNet\n",
        "NUM_STRESS_CLASSES = 4 # Based on Stress Faces Dataset (0, 1, 2, 3)\n",
        "efficientnet_model = build_efficientnet_b0(num_classes=NUM_STRESS_CLASSES)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "efficientnet_model.to(device)\n",
        "\n",
        "print(f\"\\nModels defined. EfficientNet instantiated and moved to {device}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233,
          "referenced_widgets": [
            "04086aadb0844d6794512523d5cf8576",
            "53585a84148345db91a95575f8be81a1",
            "3280eb4d24af4d70902c887466b29447",
            "b272696999c849e1abecd9a37f36f283",
            "89e3e2debaf642b8b4eba2e3de47c5d4",
            "cc5dc754f5bc4fe496d4dc7e7d19e6aa",
            "66aaa68774284374bb15a14d60b6e8f9",
            "9dfd3726a1f64a97a59186a2eb7caeca",
            "5b70efc3c8034697a1b0d3a45480b8df",
            "8c9294809cdb459fb3af6c9473042121",
            "cc37e07eae324e7499ad80bb56b612c9"
          ]
        },
        "id": "8d-f2kMu3YZs",
        "outputId": "431cda55-39ae-41fc-8288-841d23c9eb22"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/21.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "04086aadb0844d6794512523d5cf8576"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Built EfficientNet-B0 with 4 output classes.\n",
            "\n",
            "Models defined. EfficientNet instantiated and moved to cpu.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIH-rnyB3xLV",
        "outputId": "84b1894e-fa19-4df9-f22e-35fdd45e405d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.15.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.1)\n",
            "Downloading optuna-4.3.0-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.15.2-py3-none-any.whl (231 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.15.2 colorlog-6.9.0 optuna-4.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title <<< Stage 1 / Task 2.2: Training Loop & Optuna >>>\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import optuna\n",
        "from tqdm.notebook import tqdm # Progress bar\n",
        "\n",
        "# --- PyTorch Training Function ---\n",
        "def train_model_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train() # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    pbar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
        "    for inputs, labels in pbar:\n",
        "        # Skip potentially empty batches from collate_fn\n",
        "        if inputs is None or labels is None: continue\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_samples += labels.size(0)\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "        pbar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "    epoch_loss = running_loss / total_samples if total_samples > 0 else 0\n",
        "    epoch_acc = correct_predictions / total_samples if total_samples > 0 else 0\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "# --- PyTorch Validation Function ---\n",
        "def validate_model_epoch(model, dataloader, criterion, device):\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(dataloader, desc=\"Validation\", leave=False)\n",
        "        for inputs, labels in pbar:\n",
        "            if inputs is None or labels is None: continue\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_samples += labels.size(0)\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "            pbar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "    epoch_loss = running_loss / total_samples if total_samples > 0 else 0\n",
        "    epoch_acc = correct_predictions / total_samples if total_samples > 0 else 0\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "# --- Optuna Objective Function ---\n",
        "def objective(trial):\n",
        "    \"\"\"Optuna objective function for hyperparameter search.\"\"\"\n",
        "    # --- Hyperparameters to Tune ---\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
        "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
        "    # Could also tune dropout rates, weight decay, etc.\n",
        "    # Example: dropout_rate = trial.suggest_float(\"dropout\", 0.2, 0.6)\n",
        "\n",
        "    # --- Build Model (using suggested params if applicable) ---\n",
        "    # Rebuild model each trial OR modify existing model's dropout etc.\n",
        "    model = build_efficientnet_b0(num_classes=NUM_STRESS_CLASSES, pretrained=True)\n",
        "    model.to(device)\n",
        "\n",
        "    # --- Setup Optimizer ---\n",
        "    if optimizer_name == \"Adam\":\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    elif optimizer_name == \"RMSprop\":\n",
        "        optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
        "    else: # SGD\n",
        "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "    # --- Loss Function ---\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # --- DataLoaders (Assuming train_loader, val_loader are available globally) ---\n",
        "    if train_loader is None or val_loader is None:\n",
        "        print(\"Error: DataLoaders not available for Optuna trial.\")\n",
        "        return -1 # Return a value indicating failure\n",
        "\n",
        "    # --- Training Loop (Simplified - e.g., for 5 epochs) ---\n",
        "    num_epochs_optuna = 5 # Use fewer epochs for faster HPO\n",
        "    best_val_acc = 0.0\n",
        "    print(f\"\\nTrial {trial.number}: Opt={optimizer_name}, LR={lr:.6f}\")\n",
        "    for epoch in range(num_epochs_optuna):\n",
        "        train_loss, train_acc = train_model_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        val_loss, val_acc = validate_model_epoch(model, val_loader, criterion, device)\n",
        "        print(f\"  Epoch {epoch+1}/{num_epochs_optuna} - Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}\")\n",
        "\n",
        "        # --- Pruning (Optional but recommended for long HPO) ---\n",
        "        # trial.report(val_acc, epoch)\n",
        "        # if trial.should_prune():\n",
        "        #     raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "        best_val_acc = max(best_val_acc, val_acc) # Track best validation accuracy for this trial\n",
        "\n",
        "    # Return the metric to optimize (e.g., best validation accuracy)\n",
        "    return best_val_acc\n",
        "\n",
        "# --- Run Optuna Study (Example - run only a few trials) ---\n",
        "# study = optuna.create_study(direction=\"maximize\", pruner=optuna.pruners.MedianPruner())\n",
        "# study.optimize(objective, n_trials=10) # Adjust n_trials as needed\n",
        "\n",
        "# print(\"\\n--- Optuna HPO Summary ---\")\n",
        "# print(\"Best trial number:\", study.best_trial.number)\n",
        "# print(\"Best value (validation accuracy):\", study.best_value)\n",
        "# print(\"Best hyperparameters:\", study.best_params)\n",
        "\n",
        "# --- Train Final Model (using best hyperparameters found) ---\n",
        "# Re-instantiate model, optimizer with best_params, and train for full epochs\n",
        "# best_lr = study.best_params['lr']\n",
        "# best_optimizer = study.best_params['optimizer']\n",
        "# ... setup model, optimizer ...\n",
        "# ... run full training loop (e.g., 20-50 epochs) with early stopping ...\n",
        "\n",
        "print(\"✔️ PyTorch training/validation functions and Optuna objective defined.\")\n",
        "print(\"⚠️ Optuna study commented out - uncomment and run to perform HPO.\")\n",
        "print(\"   Ensure DataLoaders (train_loader, val_loader) are correctly initialized first.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgS4AKxZ3dvQ",
        "outputId": "7691c7da-e42f-4a52-d0a8-7cb217be78d7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔️ PyTorch training/validation functions and Optuna objective defined.\n",
            "⚠️ Optuna study commented out - uncomment and run to perform HPO.\n",
            "   Ensure DataLoaders (train_loader, val_loader) are correctly initialized first.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install ONNX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMiRiooc3_uy",
        "outputId": "4018db88-b89b-4411-d2fc-d9c31688f503"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ONNX\n",
            "  Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from ONNX) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from ONNX) (4.25.7)\n",
            "Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ONNX\n",
            "Successfully installed ONNX-1.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install onnxruntime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNC-R3o94HgX",
        "outputId": "6bfd9f70-9c39-4b71-8fe9-06e0c913e198"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (4.25.7)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Downloading onnxruntime-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.21.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title <<< Stage 1 / Task 2.3: Export Model to ONNX >>>\n",
        "\n",
        "import torch.onnx\n",
        "\n",
        "def export_model_to_onnx(model, dummy_input, file_path=\"stress_model.onnx\", input_names=None, output_names=None):\n",
        "    \"\"\"Exports a PyTorch model to ONNX format.\"\"\"\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    # Ensure dummy input is on the same device as the model expects (usually CPU for export)\n",
        "    # model.to('cpu') # Move model to CPU for export if needed\n",
        "    # dummy_input = dummy_input.to('cpu')\n",
        "    if input_names is None: input_names = ['input_image']\n",
        "    if output_names is None: output_names = ['stress_output']\n",
        "\n",
        "    print(f\"Exporting model to ONNX: {file_path}\")\n",
        "    try:\n",
        "        torch.onnx.export(model,               # model being run\n",
        "                          dummy_input,         # model input (or tuple for multiple inputs)\n",
        "                          file_path,           # where to save the model\n",
        "                          export_params=True,  # store the trained parameter weights inside the model file\n",
        "                          opset_version=11,    # the ONNX version to export the model to\n",
        "                          do_constant_folding=True,  # whether to execute constant folding for optimization\n",
        "                          input_names=input_names,   # the model's input names\n",
        "                          output_names=output_names, # the model's output names\n",
        "                          dynamic_axes={'input_image' : {0 : 'batch_size'}, # variable length axes\n",
        "                                        'stress_output' : {0 : 'batch_size'}})\n",
        "        print(\"✅ Model successfully exported to ONNX.\")\n",
        "        return file_path\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error exporting model to ONNX: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Example Export ---\n",
        "# Assuming 'efficientnet_model' is trained and ready\n",
        "# Need a dummy input with the correct shape (Batch, Channels, Height, Width)\n",
        "dummy_input_tensor = torch.randn(1, 3, IMG_HEIGHT, IMG_WIDTH).to(device)\n",
        "\n",
        "# Move model to CPU before exporting if it's on GPU and causing issues\n",
        "# efficientnet_model.to('cpu')\n",
        "# dummy_input_tensor = dummy_input_tensor.to('cpu')\n",
        "\n",
        "ONNX_MODEL_DIR = Path(\"./onnx_models\")\n",
        "ONNX_MODEL_DIR.mkdir(exist_ok=True)\n",
        "onnx_file_path = ONNX_MODEL_DIR / \"efficientnet_stress_model.onnx\"\n",
        "\n",
        "# Export the model (make sure model is on CPU if needed)\n",
        "exported_path = export_model_to_onnx(efficientnet_model.to('cpu'), dummy_input_tensor.to('cpu'), onnx_file_path)\n",
        "\n",
        "# Optional: Verify ONNX model\n",
        "if exported_path:\n",
        "    try:\n",
        "        import onnxruntime as ort\n",
        "        ort_session = ort.InferenceSession(exported_path, providers=['CPUExecutionProvider']) # Use CPU for verification\n",
        "        ort_inputs = {ort_session.get_inputs()[0].name: dummy_input_tensor.cpu().numpy()}\n",
        "        ort_outs = ort_session.run(None, ort_inputs)\n",
        "        print(\"✔️ ONNX model verified successfully. Output shape:\", ort_outs[0].shape)\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ ONNX model verification failed: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfjINYGi32HJ",
        "outputId": "7aa58111-2ad0-403d-b194-5b7e217bdd1c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exporting model to ONNX: onnx_models/efficientnet_stress_model.onnx\n",
            "✅ Model successfully exported to ONNX.\n",
            "✔️ ONNX model verified successfully. Output shape: (1, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install fastapi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUyvwxzt4Zjh",
        "outputId": "be4d1369-4581-49f4-fba6-853881d33eff"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastapi\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.11.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.13.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.0)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: starlette, fastapi\n",
            "Successfully installed fastapi-0.115.12 starlette-0.46.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install uvicorn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TpVlzAG4d5o",
        "outputId": "d94dcfde-aa9b-40cf-a527-deb5cff7db25"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.16.0)\n",
            "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/62.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uvicorn\n",
            "Successfully installed uvicorn-0.34.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_w98jBs64o8x",
        "outputId": "33edffeb-c49e-496c-f8e7-f4e5c5877d1c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.5-py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.5-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install ngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUPwbUz14thQ",
        "outputId": "792a04d9-9e07-4db5-c42f-99b6db9c7e86"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ngrok\n",
            "  Downloading ngrok-1.4.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Downloading ngrok-1.4.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/3.1 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/3.1 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ngrok\n",
            "Successfully installed ngrok-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title <<< Stage 1 / Task 3.1: Inference API (FastAPI + ONNX) >>>\n",
        "\n",
        "import fastapi\n",
        "import uvicorn\n",
        "import onnxruntime as ort\n",
        "from pydantic import BaseModel\n",
        "import numpy as np\n",
        "import cv2\n",
        "import base64\n",
        "import io\n",
        "import threading\n",
        "from pyngrok import ngrok\n",
        "import nest_asyncio\n",
        "import time\n",
        "from typing import List\n",
        "\n",
        "# --- Load ONNX Model ---\n",
        "onnx_model_path = exported_path # Path from the export step\n",
        "onnx_session = None\n",
        "input_name = None\n",
        "output_name = None\n",
        "\n",
        "if onnx_model_path and onnx_model_path.exists():\n",
        "    try:\n",
        "        # Prefer CUDAExecutionProvider if available and compatible ONNX Runtime installed\n",
        "        providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if torch.cuda.is_available() else ['CPUExecutionProvider']\n",
        "        onnx_session = ort.InferenceSession(str(onnx_model_path), providers=providers)\n",
        "        input_name = onnx_session.get_inputs()[0].name\n",
        "        output_name = onnx_session.get_outputs()[0].name\n",
        "        print(f\"✅ ONNX session loaded successfully using {onnx_session.get_providers()}.\")\n",
        "        print(f\"   Input: '{input_name}', Output: '{output_name}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error loading ONNX model: {e}\")\n",
        "        onnx_session = None\n",
        "else:\n",
        "    print(f\"⚠️ ONNX model file not found at {onnx_model_path}. API will not function.\")\n",
        "\n",
        "# --- API Application ---\n",
        "app_inf = fastapi.FastAPI(title=\"Stress Detection Inference API\")\n",
        "\n",
        "# Use Pydantic models defined earlier (FrameInput, VideoInput)\n",
        "class StressPrediction(BaseModel):\n",
        "    stress_category: int\n",
        "    stress_probabilities: List[float] # Probabilities for each category\n",
        "    confidence: float\n",
        "    message: Optional[str] = None\n",
        "\n",
        "# Preprocessing function for inference (similar to validation transform)\n",
        "# Note: Assumes input is a BGR image from OpenCV\n",
        "infer_transform = A.Compose([\n",
        "    A.Resize(height=IMG_HEIGHT, width=IMG_WIDTH, interpolation=cv2.INTER_LINEAR),\n",
        "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ToTensorV2() # Output: PyTorch Tensor C, H, W\n",
        "])\n",
        "\n",
        "def preprocess_inference_image(image_bgr: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Preprocesses a single BGR image for ONNX inference.\"\"\"\n",
        "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
        "    augmented = infer_transform(image=image_rgb)\n",
        "    # Convert tensor to numpy C, H, W -> add batch dim -> B, C, H, W\n",
        "    input_tensor = augmented['image'].unsqueeze(0).cpu().numpy()\n",
        "    return input_tensor\n",
        "\n",
        "@app_inf.post(\"/predict/frame\", response_model=StressPrediction)\n",
        "async def predict_single_frame(frame_input: FrameInput):\n",
        "    \"\"\"Predicts stress from a single base64 encoded image frame.\"\"\"\n",
        "    if not onnx_session:\n",
        "        raise fastapi.HTTPException(status_code=503, detail=\"ONNX Model not loaded\")\n",
        "\n",
        "    try:\n",
        "        # Decode image\n",
        "        img_bytes = base64.b64decode(frame_input.image_b64, validate=True)\n",
        "        img_bgr = cv2.imdecode(np.frombuffer(img_bytes, np.uint8), cv2.IMREAD_COLOR)\n",
        "        if img_bgr is None:\n",
        "            raise fastapi.HTTPException(status_code=400, detail=\"Invalid image data\")\n",
        "\n",
        "        # Optional: Face detection here if needed\n",
        "        # landmarks, bbox, face_crop = detect_face_and_landmarks_mediapipe(img_bgr)\n",
        "        # if face_crop is None:\n",
        "        #    raise fastapi.HTTPException(status_code=400, detail=\"Face not detected\")\n",
        "        # input_image = face_crop # Use cropped face\n",
        "        input_image = img_bgr # Use full image if no cropping needed/done\n",
        "\n",
        "        # Preprocess\n",
        "        input_tensor = preprocess_inference_image(input_image)\n",
        "\n",
        "        # Run ONNX inference\n",
        "        ort_inputs = {input_name: input_tensor}\n",
        "        ort_outs = onnx_session.run([output_name], ort_inputs)\n",
        "        logits = ort_outs[0][0] # Output shape (batch_size, num_classes) -> get first batch\n",
        "\n",
        "        # Post-process: Apply Softmax for probabilities\n",
        "        probabilities = torch.softmax(torch.tensor(logits), dim=-1).tolist()\n",
        "        predicted_category = int(np.argmax(logits))\n",
        "        confidence = float(max(probabilities))\n",
        "\n",
        "        return StressPrediction(\n",
        "            stress_category=predicted_category,\n",
        "            stress_probabilities=probabilities,\n",
        "            confidence=confidence,\n",
        "            message=\"Prediction successful\"\n",
        "        )\n",
        "    except ValueError as e: # Catch base64 errors etc.\n",
        "         raise fastapi.HTTPException(status_code=400, detail=f\"Bad Request: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during prediction: {e}\")\n",
        "        raise fastapi.HTTPException(status_code=500, detail=f\"Internal Server Error: {e}\")\n",
        "\n",
        "\n",
        "@app_inf.post(\"/predict/video\") # Add response model later\n",
        "async def predict_video_clip(video_input: VideoInput):\n",
        "    \"\"\"\n",
        "    Predicts stress from a sequence of frames (video clip).\n",
        "    Placeholder: Needs rPPG integration and sequence model inference.\n",
        "    \"\"\"\n",
        "    if not onnx_session: # Add checks for sequence model later\n",
        "        raise fastapi.HTTPException(status_code=503, detail=\"Model not loaded\")\n",
        "\n",
        "    if len(video_input.frames) < 10: # Example minimum length\n",
        "         raise fastapi.HTTPException(status_code=400, detail=\"Insufficient frames for video analysis.\")\n",
        "\n",
        "    frames_bgr = []\n",
        "    landmarks_seq = []\n",
        "    timestamps = []\n",
        "    h, w = -1, -1 # Get frame dimensions\n",
        "\n",
        "    # 1. Decode and detect landmarks for all frames\n",
        "    for frame_data in video_input.frames:\n",
        "        try:\n",
        "            img_bytes = base64.b64decode(frame_data.image_b64, validate=True)\n",
        "            img = cv2.imdecode(np.frombuffer(img_bytes, np.uint8), cv2.IMREAD_COLOR)\n",
        "            if img is None: continue # Skip bad frames\n",
        "\n",
        "            if h == -1: h, w = img.shape[:2]\n",
        "            frames_bgr.append(img)\n",
        "            timestamps.append(frame_data.timestamp if frame_data.timestamp else time.time())\n",
        "\n",
        "            # Detect landmarks for rPPG ROI\n",
        "            landmarks, _, _ = detect_face_and_landmarks_mediapipe(img)\n",
        "            landmarks_seq.append(landmarks) # Store landmarks (can be None)\n",
        "\n",
        "        except Exception:\n",
        "            # Handle decoding/detection errors for individual frames\n",
        "            landmarks_seq.append(None) # Keep sequences aligned\n",
        "            continue\n",
        "\n",
        "    if not frames_bgr:\n",
        "         raise fastapi.HTTPException(status_code=400, detail=\"No valid frames processed.\")\n",
        "\n",
        "    # 2. Estimate rPPG (if fps provided and enough frames)\n",
        "    estimated_hr = None\n",
        "    if video_input.fps and len(frames_bgr) >= int(video_input.fps * 2):\n",
        "        rois_seq = select_rois(landmarks_seq, (h, w), LEFT_CHEEK_IDXS + RIGHT_CHEEK_IDXS) # Combine cheek ROIs\n",
        "        mean_rgb_signal = extract_mean_rgb(frames_bgr, rois_seq)\n",
        "        estimated_hr = estimate_rppg_chrom(mean_rgb_signal, video_input.fps)\n",
        "        print(f\"Estimated HR (rPPG): {estimated_hr} BPM\")\n",
        "\n",
        "\n",
        "    # 3. Feature Extraction / Sequence Model Inference (Placeholder)\n",
        "    # If using EfficientNet (frame-based): Predict per frame, then aggregate (e.g., average, max pooling)\n",
        "    # If using ResNet+GRU/MobileNet+LSTM: Preprocess sequence, run ONNX sequence model.\n",
        "    # This part needs the sequence model to be trained and exported.\n",
        "    aggregated_stress_category = random.randint(0, NUM_STRESS_CLASSES - 1) # Placeholder\n",
        "    aggregated_probabilities = [0.25] * NUM_STRESS_CLASSES # Placeholder\n",
        "\n",
        "\n",
        "    return {\n",
        "        \"aggregated_stress_category\": aggregated_stress_category,\n",
        "        \"stress_probabilities\": aggregated_probabilities,\n",
        "        \"estimated_hr_bpm\": estimated_hr,\n",
        "        \"num_frames_processed\": len(frames_bgr),\n",
        "        \"message\": \"Video prediction placeholder (rPPG estimated if possible)\"\n",
        "    }\n",
        "\n",
        "\n",
        "# --- Run FastAPI app (in background thread for Colab) ---\n",
        "def run_api():\n",
        "    nest_asyncio.apply()\n",
        "    port = 8001 # Use a different port than the main backend if running simultaneously\n",
        "    try:\n",
        "        ngrok.kill() # Kill previous tunnels if any\n",
        "        public_url = ngrok.connect(port)\n",
        "        print(f\"🚀 Inference API running on: {public_url}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Ngrok connection failed: {e}\")\n",
        "        print(\"Ensure ngrok is installed and authenticated if needed.\")\n",
        "        public_url = f\"http://127.0.0.1:{port}\" # Fallback to local URL\n",
        "\n",
        "    uvicorn.run(app_inf, host=\"0.0.0.0\", port=port, log_level=\"info\")\n",
        "\n",
        "print(\"\\n--- Starting Inference API ---\")\n",
        "# Check if ONNX model loaded before starting\n",
        "if onnx_session:\n",
        "    api_thread = threading.Thread(target=run_api, daemon=True)\n",
        "    api_thread.start()\n",
        "    time.sleep(5) # Allow time for server to start\n",
        "    print(\"✔️ Inference API started in background.\")\n",
        "else:\n",
        "    print(\"❌ Inference API cannot start - ONNX model not loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Km1a5wvR4QLC",
        "outputId": "0f8ebcc2-4dd0-4d7c-df81-cce9aebf196c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ ONNX session loaded successfully using ['CPUExecutionProvider'].\n",
            "   Input: 'input_image', Output: 'stress_output'\n",
            "\n",
            "--- Starting Inference API ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:pyngrok.process.ngrok:t=2025-05-04T10:36:11+0000 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2025-05-04T10:36:11+0000 lvl=eror msg=\"session closing\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2025-05-04T10:36:11+0000 lvl=eror msg=\"terminating with error\" obj=app err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "CRITICAL:pyngrok.process.ngrok:t=2025-05-04T10:36:11+0000 lvl=crit msg=\"command failed\" err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "INFO:     Started server process [3968]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ngrok connection failed: The ngrok process errored on start: authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n.\n",
            "Ensure ngrok is installed and authenticated if needed.\n",
            "✔️ Inference API started in background.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title <<< Stage 1 / Task 3.2: Explainability (Grad-CAM) >>>\n",
        "\n",
        "# Install grad-cam library\n",
        "!pip install -q grad-cam opencv-python matplotlib\n",
        "\n",
        "from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image, \\\n",
        "                                         deprocess_image, \\\n",
        "                                         preprocess_image\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "def visualize_grad_cam(model_pytorch, target_layer, input_tensor_normalized, target_category=None, use_cuda=False):\n",
        "    \"\"\"Generates and displays Grad-CAM visualization.\"\"\"\n",
        "    # Ensure model is on the correct device and in eval mode\n",
        "    device = torch.device(\"cuda\" if use_cuda and torch.cuda.is_available() else \"cpu\")\n",
        "    model_pytorch.to(device)\n",
        "    model_pytorch.eval()\n",
        "\n",
        "    # Construct the CAM object\n",
        "    # Available methods: GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, etc.\n",
        "    cam = GradCAM(model=model_pytorch, target_layers=[target_layer], use_cuda=use_cuda)\n",
        "\n",
        "    # Define targets (optional: focus on a specific class)\n",
        "    targets = None\n",
        "    if target_category is not None:\n",
        "        targets = [ClassifierOutputTarget(target_category)]\n",
        "\n",
        "    # Generate CAM mask\n",
        "    # Input tensor needs to be B, C, H, W and on the correct device\n",
        "    input_tensor_cam = input_tensor_normalized.to(device)\n",
        "    grayscale_cam = cam(input_tensor=input_tensor_cam, targets=targets)\n",
        "\n",
        "    # Take the first image in the batch\n",
        "    grayscale_cam = grayscale_cam[0, :]\n",
        "\n",
        "    # --- Visualization ---\n",
        "    # Need the *original* image (before normalization) for overlay\n",
        "    # We need to 'deprocess' the normalized tensor back to an image-like format\n",
        "    # NOTE: This deprocessing might not perfectly recover the original image appearance\n",
        "    # but is standard for visualization.\n",
        "    img_deprocessed = deprocess_image(input_tensor_normalized.cpu().numpy()[0]) # H, W, C\n",
        "\n",
        "    # Overlay CAM\n",
        "    visualization = show_cam_on_image(img_deprocessed, grayscale_cam, use_rgb=True)\n",
        "\n",
        "    # Display\n",
        "    plt.imshow(visualization)\n",
        "    plt.title(f\"Grad-CAM (Target: {'Auto' if target_category is None else target_category})\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    return visualization\n",
        "\n",
        "\n",
        "# --- Example Usage ---\n",
        "# Requires the trained PyTorch model ('efficientnet_model' from earlier)\n",
        "# And a sample input tensor (use one from DataLoader or preprocess a sample image)\n",
        "\n",
        "# 1. Find a target convolutional layer in the PyTorch model\n",
        "# Example for EfficientNet-B0 in timm: Inspect model structure\n",
        "# print(efficientnet_model)\n",
        "# Common target layers are the last conv block, e.g., `model.blocks[-1].conv_pwl` or similar\n",
        "# Let's assume a target layer - *replace with actual layer from your model*\n",
        "#target_layer_example = efficientnet_model.blocks[-1].conv_pw # Adjust based on print(model) output\n",
        "#target_layer_example = efficientnet_model.features[-1]\n",
        "# Modified line: Get the target layer from 'blocks' instead of 'features'\n",
        "target_layer_example = efficientnet_model.blocks[-1][-1].conv_pwl # Adjust based on print(model) output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 2. Get a sample input tensor (normalized, B C H W)\n",
        "# Example: Get a sample from the validation loader\n",
        "if 'val_loader' in locals() and val_loader:\n",
        "    try:\n",
        "      sample_inputs, sample_labels = next(iter(val_loader))\n",
        "      if sample_inputs is None: raise StopIteration(\"Empty batch received\")\n",
        "      sample_input_tensor = sample_inputs[0:1] # Take first image in batch -> (1, C, H, W)\n",
        "\n",
        "      # 3. Run Grad-CAM visualization\n",
        "      print(\"\\n--- Generating Grad-CAM Example ---\")\n",
        "      _ = visualize_grad_cam(\n",
        "          model_pytorch=efficientnet_model,\n",
        "          target_layer=target_layer_example,\n",
        "          input_tensor_normalized=sample_input_tensor,\n",
        "          # target_category=sample_labels[0].item(), # Optional: Target the true label\n",
        "          use_cuda=torch.cuda.is_available()\n",
        "      )\n",
        "      print(\"✔️ Grad-CAM visualization example shown.\")\n",
        "\n",
        "    except Exception as e:\n",
        "       print(f\"⚠️ Could not run Grad-CAM example: {e}\")\n",
        "       if 'val_loader' not in locals() or not val_loader: print(\"   (Validation loader not available)\")\n",
        "else:\n",
        "   print(\"⚠️ Skipping Grad-CAM example (Validation loader not available).\")\n",
        "\n",
        "\n",
        "print(\"✔️ Grad-CAM function defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOBpxtJU4zmX",
        "outputId": "13d95a34-16c7-4d97-ff53-6c0d52c86655"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Skipping Grad-CAM example (Validation loader not available).\n",
            "✔️ Grad-CAM function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: 3.3. Bias and Fairness Evaluation Strategy\n",
        "# Data: Requires test datasets annotated with subgroup information (e.g., gender, age group, ethnicity). Datasets like FER+ or specialized bias datasets might be needed. If using FER2013/AffectNet/RAF-DB, check if any such metadata exists or can be inferred (often difficult/unreliable). The \"Stress Faces Dataset\" likely lacks this. Action: Identify or create a suitably annotated test set.\n",
        "# Methodology:\n",
        "# Define Subgroups: Clearly define the relevant subgroups for evaluation based on available data and potential bias concerns in facial analysis (e.g., Male/Female, Young/Middle/Old, different ethnicities if available).\n",
        "# Segment Test Set: Divide the annotated test set based on these subgroups.\n",
        "# Calculate Metrics per Subgroup: Run the trained model (e.g., the exported ONNX model or the PyTorch model) on each subgroup subset. Calculate standard performance metrics (Accuracy, Precision, Recall, F1-score, ROC AUC if applicable) separately for each group.\n",
        "# Compare Metrics: Analyze the results. Look for statistically significant differences in performance between subgroups. For example, does the model perform much worse for females than males? Or for darker skin tones vs. lighter ones?\n",
        "# Tools: Libraries like fairlearn can help analyze and visualize fairness metrics (e.g., demographic parity, equalized odds).\n",
        "# Reporting: Document the fairness evaluation results clearly, highlighting any identified biases.\n",
        "# Mitigation (If Bias Detected):\n",
        "# Data Augmentation: Increase representation of underperforming subgroups in the training data (e.g., using GANs or targeted augmentation).\n",
        "# Algorithmic Adjustments: Techniques like re-weighting the loss function during training, adversarial debiasing, or post-processing adjustments to predictions (use with caution).\n",
        "# Re-evaluation: After applying mitigation, re-run the fairness evaluation.\n",
        "# Python\n",
        "# # @title <<< Stage 1 / Task 3.3: Bias and Fairness Strategy >>>\n",
        "# print(\"\"\"\n",
        "# --- Bias and Fairness Evaluation Strategy -\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Sample data (replace with your actual data loading logic)\n",
        "fer2013_data = pd.read_csv(\"data/fer2013/fer2013.csv\")\n",
        "\n",
        "# Example: Extract gender information (if available in the dataset)\n",
        "# This assumes 'Usage' column and 'gender' column are present\n",
        "# Adjust based on actual column names and preprocessing\n",
        "# if 'Usage' in fer2013_data.columns and 'gender' in fer2013_data.columns:\n",
        "#     fer2013_public_test = fer2013_data[fer2013_data[\"Usage\"] == \"PublicTest\"]\n",
        "#     gender_counts = fer2013_public_test[\"gender\"].value_counts()\n",
        "#     print(gender_counts)\n",
        "\n",
        "# Example of subgroup definitions\n",
        "# These are just examples, adapt them to the actual subgroups and metadata in your dataset\n",
        "subgroups = {\n",
        "    \"gender\": [\"Male\", \"Female\"],  # If gender info is available\n",
        "    \"age\": [\"Young\", \"Middle-aged\", \"Old\"], # If age group info is available\n",
        "}\n",
        "\n",
        "# Placeholder for analysis (replace with your actual analysis)\n",
        "def analyze_bias():\n",
        "    # Load your dataset and define subgroups as needed\n",
        "    print(\"Bias analysis (placeholder)\")\n",
        "    # ... your analysis logic to calculate metrics and visualize fairness ...\n",
        "\n",
        "# Call the function\n",
        "analyze_bias()\n",
        "\n",
        "print(\"Bias and Fairness Evaluation Strategy completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6RBCZjo565U",
        "outputId": "2c8071dd-8f22-41a3-b60f-0facba42d9a1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bias analysis (placeholder)\n",
            "Bias and Fairness Evaluation Strategy completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "waxaG7R16N6T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}