{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NN3gOSxbMseq"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "AI-Driven Doctor Stress Detection and Shift Optimization Platform\n",
        "\n",
        "This notebook implements an end-to-end system for:\n",
        "1. Detecting doctor stress levels from facial expressions using deep learning.\n",
        "2. Optimizing doctor shifts based on stress levels and operational constraints.\n",
        "\n",
        "Author: Gemini (Acting as Senior Software Engineer & AI Expert)\n",
        "Date: May 3, 2025\n",
        "\"\"\"\n",
        "\n",
        "# @title << 0. Setup: Install Dependencies and Configure APIs >>\n",
        "# Install necessary libraries\n",
        "!pip install -q kaggle pandas numpy scikit-learn tensorflow tf2onnx onnxruntime-gpu fastapi uvicorn pyngrok nest-asyncio pydantic[email] Pillow requests matplotlib seaborn opencv-python-headless mediapipe google-generativeai python-dotenv ortools albumentations ray[tune] optuna sqlalchemy pyodbc\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import zipfile\n",
        "import random\n",
        "import shutil\n",
        "import time\n",
        "import datetime\n",
        "import asyncio\n",
        "import threading\n",
        "from pathlib import Path\n",
        "from getpass import getpass # For safer key handling in Colab\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "\n",
        "# Deep Learning / ML\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, GRU, LSTM, TimeDistributed, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import ResNet50, EfficientNetB0, MobileNetV3Small\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import albumentations as A\n",
        "import mediapipe as mp\n",
        "# import optuna # Or Ray Tune - Placeholder for HPO\n",
        "# import tf2onnx\n",
        "# import onnxruntime as ort\n",
        "# import tensorrt as trt # Placeholder - Requires specific environment\n",
        "\n",
        "# Shift Optimization\n",
        "from ortools.sat.python import cp_model\n",
        "\n",
        "# Backend & API\n",
        "import fastapi\n",
        "import uvicorn\n",
        "from pydantic import BaseModel, EmailStr, Field\n",
        "from pyngrok import ngrok # To expose Colab API endpoint\n",
        "import nest_asyncio\n",
        "\n",
        "# Gemini API\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Database (conceptual schema)\n",
        "import sqlalchemy as db\n",
        "from sqlalchemy import create_engine, MetaData, Table, Column, Integer, String, DateTime, Float, ForeignKey\n",
        "\n",
        "# Plotting and Utils\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import PIL.Image\n",
        "\n",
        "print(\"TensorFlow Version:\", tf.__version__)\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "# Ensure GPU is utilized if available\n",
        "if len(tf.config.list_physical_devices('GPU')) > 0:\n",
        "    print(\"Using GPU\")\n",
        "    # Optional: Set memory growth to avoid allocating all GPU memory at once\n",
        "    physical_devices = tf.config.list_physical_devices('GPU')\n",
        "    try:\n",
        "        for gpu in physical_devices:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e:\n",
        "        # Memory growth must be set before GPUs have been initialized\n",
        "        print(e)\n",
        "else:\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "# --- Configuration ---\n",
        "\n",
        "# !! IMPORTANT SECURITY NOTE !!\n",
        "# Hardcoding API keys directly in code (as provided in the prompt) is highly insecure.\n",
        "# In a real application, use environment variables, secret managers (like GCP Secret Manager or AWS Secrets Manager),\n",
        "# or secure configuration files. For Colab, using Colab Secrets or getpass is better.\n",
        "\n",
        "# Using placeholders - replace with your actual credentials or use Colab Secrets\n",
        "# KAGGLE_USERNAME = \"YOUR_KAGGLE_USERNAME\" # Replace or use Colab secrets\n",
        "# KAGGLE_KEY = \"YOUR_KAGGLE_API_KEY\"      # Replace or use Colab secrets\n",
        "# GEMINI_API_KEY = \"YOUR_GEMINI_API_KEY\"  # Replace or use Colab secrets\n",
        "\n",
        "# Using getpass for slightly better security in demo:\n",
        "if 'KAGGLE_USERNAME' not in os.environ:\n",
        "  os.environ['KAGGLE_USERNAME'] = getpass('Enter Kaggle Username: ')\n",
        "if 'KAGGLE_KEY' not in os.environ:\n",
        "  os.environ['KAGGLE_KEY'] = getpass('Enter Kaggle Key: ')\n",
        "if 'GEMINI_API_KEY' not in os.environ:\n",
        "  # Securely store the Gemini API key\n",
        "  try:\n",
        "      from google.colab import userdata\n",
        "      GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "      if not GEMINI_API_KEY:\n",
        "          GEMINI_API_KEY = getpass('Enter Gemini API Key: ')\n",
        "          print(\"Gemini Key entered via getpass.\")\n",
        "      else:\n",
        "          print(\"Gemini Key loaded from Colab Secrets.\")\n",
        "  except ImportError:\n",
        "      GEMINI_API_KEY = getpass('Enter Gemini API Key: ')\n",
        "      print(\"Gemini Key entered via getpass (Colab Secrets not available).\")\n",
        "  genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "\n",
        "# --- Kaggle API Setup ---\n",
        "KAGGLE_CONFIG_DIR = os.path.join(Path.home(), '.kaggle')\n",
        "os.makedirs(KAGGLE_CONFIG_DIR, exist_ok=True)\n",
        "KAGGLE_JSON_PATH = os.path.join(KAGGLE_CONFIG_DIR, 'kaggle.json')\n",
        "\n",
        "# Write kaggle.json if credentials were provided via getpass or env\n",
        "if 'KAGGLE_USERNAME' in os.environ and 'KAGGLE_KEY' in os.environ:\n",
        "    kaggle_credentials = {\n",
        "        \"username\": os.environ['KAGGLE_USERNAME'],\n",
        "        \"key\": os.environ['KAGGLE_KEY']\n",
        "    }\n",
        "    with open(KAGGLE_JSON_PATH, 'w') as f:\n",
        "        json.dump(kaggle_credentials, f)\n",
        "    os.chmod(KAGGLE_JSON_PATH, 600) # Set permissions\n",
        "    print(\"Kaggle API configured.\")\n",
        "else:\n",
        "    print(\"Kaggle credentials not found. Please provide them or upload kaggle.json manually.\")\n",
        "\n",
        "# --- Constants ---\n",
        "DATA_DIR = Path(\"./data\")\n",
        "MODEL_DIR = Path(\"./models\")\n",
        "ONNX_MODEL_DIR = Path(\"./onnx_models\")\n",
        "LOG_DIR = Path(\"./logs\")\n",
        "IMG_SIZE = (224, 224) # Input size for models like ResNet, EfficientNet\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 15 # Reduced for demo purposes; increase for real training (e.g., 50-100)\n",
        "SEED = 42\n",
        "STRESS_THRESHOLD = 0.6 # Example threshold for triggering alerts/optimization\n",
        "\n",
        "# Create directories\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "MODEL_DIR.mkdir(exist_ok=True)\n",
        "ONNX_MODEL_DIR.mkdir(exist_ok=True)\n",
        "LOG_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title << 1. Data Collection & Preprocessing >>\n",
        "\n",
        "# --- 1.1 Dataset Download ---\n",
        "# Define datasets to use. We'll use FER2013 as a base for emotions,\n",
        "# and simulate or find a smaller stress-specific dataset.\n",
        "# NOTE: Finding large, public, ethically sourced facial *stress* datasets is challenging.\n",
        "# We will use FER2013 for emotion recognition which can be a *proxy* or feature for stress,\n",
        "# and supplement with simulated data or a smaller dataset if available.\n",
        "\n",
        "# Dataset Links (as requested):\n",
        "# FER2013: https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data (Requires joining competition)\n",
        "# AffectNet: http://mohammadmahoor.com/affectnet/ (Requires license agreement)\n",
        "# RAF-DB: http://www.whdeng.cn/RAF/model1.html (Requires license agreement)\n",
        "# Simulated Stress/Custom Data: Often needed for specific stress detection.\n",
        "\n",
        "# Example: Download FER2013\n",
        "# You might need to accept competition rules on Kaggle first.\n",
        "print(\"Downloading FER2013 dataset...\")\n",
        "!kaggle competitions download -c challenges-in-representation-learning-facial-expression-recognition-challenge -p {DATA_DIR} --force\n",
        "\n",
        "fer2013_zip = DATA_DIR / \"challenges-in-representation-learning-facial-expression-recognition-challenge.zip\"\n",
        "fer2013_dir = DATA_DIR / \"fer2013\"\n",
        "\n",
        "if fer2013_zip.exists():\n",
        "    print(\"Extracting FER2013...\")\n",
        "    with zipfile.ZipFile(fer2013_zip, 'r') as zip_ref:\n",
        "        zip_ref.extractall(fer2013_dir)\n",
        "    # Clean up zip file\n",
        "    # os.remove(fer2013_zip) # Keep the zip for record if needed\n",
        "    print(\"FER2013 extracted.\")\n",
        "    # Check for the actual data file (often fer2013.csv)\n",
        "    if not (fer2013_dir / 'fer2013.csv').exists():\n",
        "         print(f\"Warning: Expected 'fer2013.csv' not found in {fer2013_dir}. Check extraction.\")\n",
        "         # Attempt to find it in subdirectories if necessary\n",
        "         potential_csv = list(fer2013_dir.glob('**/fer2013.csv'))\n",
        "         if potential_csv:\n",
        "             # Move csv to the main fer2013_dir for consistency\n",
        "             shutil.move(str(potential_csv[0]), str(fer2013_dir / 'fer2013.csv'))\n",
        "             print(f\"Moved {potential_csv[0]} to {fer2013_dir}\")\n",
        "         else:\n",
        "             print(\"Could not locate fer2013.csv. Manual check required.\")\n",
        "\n",
        "else:\n",
        "    print(\"FER2013 download failed. Ensure you have joined the competition on Kaggle and API key is correct.\")\n",
        "    # As a fallback, we'll create dummy data structure later if download fails\n",
        "\n",
        "# Placeholder for other datasets (AffectNet, RAF-DB, Stress Faces)\n",
        "# These often require manual download and agreement forms.\n",
        "# Example structure if downloaded manually:\n",
        "# (DATA_DIR / \"AffectNet\").mkdir(exist_ok=True)\n",
        "# (DATA_DIR / \"RAF-DB\").mkdir(exist_ok=True)\n",
        "# (DATA_DIR / \"StressFaces\").mkdir(exist_ok=True)\n",
        "print(\"Placeholder directories created for AffectNet, RAF-DB, StressFaces.\")\n",
        "print(\"Please download these manually if needed, respecting their licenses.\")\n",
        "\n",
        "\n",
        "# --- 1.2 Data Loading and Initial Exploration (FER2013 Example) ---\n",
        "fer_csv_path = fer2013_dir / 'fer2013.csv'\n",
        "if fer_csv_path.exists():\n",
        "    print(f\"Loading {fer_csv_path}...\")\n",
        "    try:\n",
        "        fer_df = pd.read_csv(fer_csv_path)\n",
        "        print(\"FER2013 Dataframe Info:\")\n",
        "        fer_df.info()\n",
        "        print(\"\\nFER2013 Emotion Distribution:\")\n",
        "        print(fer_df['emotion'].value_counts())\n",
        "        # Emotion mapping: 0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral\n",
        "        emotion_map = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}\n",
        "        fer_df['emotion_label'] = fer_df['emotion'].map(emotion_map)\n",
        "\n",
        "        # Display some sample images\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        for i in range(5):\n",
        "            idx = random.randint(0, len(fer_df) - 1)\n",
        "            pixels = np.array(fer_df['pixels'][idx].split(), dtype='uint8')\n",
        "            img = pixels.reshape(48, 48)\n",
        "            plt.subplot(1, 5, i + 1)\n",
        "            plt.imshow(img, cmap='gray')\n",
        "            plt.title(f\"Emotion: {fer_df['emotion_label'][idx]}\")\n",
        "            plt.axis('off')\n",
        "        plt.suptitle(\"Sample FER2013 Images\")\n",
        "        plt.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading or processing FER2013 CSV: {e}\")\n",
        "        fer_df = None # Indicate failure\n",
        "else:\n",
        "    print(f\"FER2013 CSV not found at {fer_csv_path}. Cannot load data.\")\n",
        "    fer_df = None\n",
        "\n",
        "\n",
        "# --- 1.3 Preprocessing Functions ---\n",
        "\n",
        "# Face Detection (using MediaPipe)\n",
        "mp_face_detection = mp.solutions.face_detection\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "face_detector = mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5)\n",
        "\n",
        "def detect_face(image_bgr):\n",
        "    \"\"\"Detects the largest face in an image using MediaPipe.\"\"\"\n",
        "    results = face_detector.process(cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB))\n",
        "    if not results.detections:\n",
        "        return None, None # No face detected\n",
        "\n",
        "    # Find the detection with the largest bounding box area\n",
        "    best_detection = None\n",
        "    max_area = 0\n",
        "    ih, iw, _ = image_bgr.shape\n",
        "\n",
        "    for detection in results.detections:\n",
        "        bboxC = detection.location_data.relative_bounding_box\n",
        "        bbox = int(bboxC.xmin * iw), int(bboxC.ymin * ih), \\\n",
        "               int(bboxC.width * iw), int(bboxC.height * ih)\n",
        "        area = bbox[2] * bbox[3]\n",
        "        if area > max_area:\n",
        "            max_area = area\n",
        "            best_detection = bbox\n",
        "\n",
        "    x, y, w, h = best_detection\n",
        "    # Ensure bbox coordinates are within image bounds and valid\n",
        "    x = max(0, x)\n",
        "    y = max(0, y)\n",
        "    w = min(iw - x, w)\n",
        "    h = min(ih - y, h)\n",
        "    if w <= 0 or h <= 0:\n",
        "        return None, None # Invalid bbox dimensions\n",
        "\n",
        "    face_img = image_bgr[y:y+h, x:x+w]\n",
        "    return face_img, best_detection\n",
        "\n",
        "# Landmark Extraction (using MediaPipe Face Mesh) - Optional but useful feature\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, min_detection_confidence=0.5)\n",
        "\n",
        "def extract_landmarks(image_rgb):\n",
        "    \"\"\"Extracts facial landmarks using MediaPipe Face Mesh.\"\"\"\n",
        "    results = face_mesh.process(image_rgb)\n",
        "    if not results.multi_face_landmarks:\n",
        "        return None\n",
        "    # Assuming only one face\n",
        "    landmarks = results.multi_face_landmarks[0].landmark\n",
        "    # Convert landmarks to numpy array (x, y, z) - z is depth estimate\n",
        "    landmark_points = np.array([[lm.x, lm.y, lm.z] for lm in landmarks])\n",
        "    return landmark_points\n",
        "\n",
        "# rPPG Estimation (Placeholder - Very Complex)\n",
        "# Real rPPG requires analyzing subtle color changes in skin over time (video).\n",
        "# Requires advanced signal processing (e.g., ICA, CHROM, POS methods).\n",
        "# Here, we'll just have a placeholder function.\n",
        "def estimate_rppg(face_video_frames):\n",
        "    \"\"\"Placeholder for rPPG estimation. Returns a simulated heart rate variability metric.\"\"\"\n",
        "    # In reality: process frame sequence, ROI selection (forehead/cheeks),\n",
        "    # color channel analysis, filtering, FFT/peak detection.\n",
        "    print(\"Warning: rPPG estimation is complex and simulated here.\")\n",
        "    # Simulate some HRV metric based on number of frames or random noise\n",
        "    simulated_sdnn = np.random.uniform(30, 80) # Example: SDNN (standard deviation of NN intervals)\n",
        "    return simulated_sdnn\n",
        "\n",
        "# Normalization and Resizing\n",
        "def preprocess_image(image, target_size):\n",
        "    \"\"\"Resizes and normalizes image.\"\"\"\n",
        "    img_resized = cv2.resize(image, target_size)\n",
        "    img_normalized = img_resized / 255.0 # Scale pixels to [0, 1]\n",
        "    return img_normalized.astype(np.float32)\n",
        "\n",
        "\n",
        "# --- 1.4 Data Augmentation ---\n",
        "# Using Albumentations for flexibility\n",
        "# Geometric + Photometric\n",
        "transform_train = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.Rotate(limit=15, p=0.5, border_mode=cv2.BORDER_CONSTANT),\n",
        "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.6),\n",
        "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.4),\n",
        "    A.MotionBlur(blur_limit=7, p=0.3),\n",
        "    # A.CoarseDropout(max_holes=8, max_height=8, max_width=8, min_holes=1, min_height=4, min_width=4, p=0.3), # Cutout\n",
        "    # Add more augmentations as needed\n",
        "])\n",
        "\n",
        "# Basic transform for validation/test (only resizing/normalization is done in preprocess_image)\n",
        "transform_val = A.Compose([]) # Usually no augmentation for validation/test\n",
        "\n",
        "# Temporal Smoothing (Conceptual - Applied during sequence processing)\n",
        "# Could involve averaging features/predictions over a short time window.\n",
        "\n",
        "# GAN-based Oversampling (Advanced - Placeholder)\n",
        "# Requires training a GAN (e.g., StyleGAN, Diffusers) on specific stress expressions\n",
        "# or using augmentation techniques like MixAugment.\n",
        "def apply_mixaugment(images, labels):\n",
        "    \"\"\"Placeholder for GAN-based/advanced oversampling.\"\"\"\n",
        "    print(\"Note: GAN-based oversampling (MixAugment/Diffusion) requires separate complex setup.\")\n",
        "    # Could involve generating synthetic stressed faces or interpolating between samples.\n",
        "    return images, labels\n",
        "\n",
        "# --- 1.5 Prepare Data for Models ---\n",
        "# Process FER2013 (if loaded) into images and labels suitable for TF/Keras\n",
        "# We will map FER emotions to a simplified Stress/NoStress binary classification for this demo.\n",
        "# Mapping: Angry, Fear, Sad -> Stress (1); Happy, Neutral, Surprise -> NoStress (0); Disgust -> Exclude (often low samples)\n",
        "\n",
        "def prepare_fer_data(df, target_size, stress_map, use_face_detection=True):\n",
        "    images = []\n",
        "    labels = []\n",
        "    skipped_count = 0\n",
        "\n",
        "    if df is None:\n",
        "      print(\"FER DataFrame is None. Skipping preparation.\")\n",
        "      return np.array([]), np.array([]) # Return empty arrays\n",
        "\n",
        "    print(f\"Preparing data with target size {target_size}...\")\n",
        "    for index, row in df.iterrows():\n",
        "        emotion = row['emotion']\n",
        "        if emotion == 1: # Skip 'Disgust'\n",
        "             skipped_count += 1\n",
        "             continue\n",
        "\n",
        "        target_label = stress_map.get(emotion)\n",
        "        if target_label is None: # Should not happen with the map defined below\n",
        "            skipped_count += 1\n",
        "            continue\n",
        "\n",
        "        pixels = np.array(row['pixels'].split(), dtype='uint8')\n",
        "        img = pixels.reshape(48, 48)\n",
        "        img_bgr = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR) # Convert to 3 channels for models like ResNet\n",
        "\n",
        "        face = img_bgr # Default to original if face detection fails or is off\n",
        "        if use_face_detection:\n",
        "            detected_face, _ = detect_face(img_bgr)\n",
        "            if detected_face is not None and detected_face.size > 0:\n",
        "                face = detected_face\n",
        "            else:\n",
        "                # Optional: Skip if face not detected, or use original\n",
        "                # skipped_count += 1\n",
        "                # continue\n",
        "                pass # Use original 48x48 converted image if face detection fails\n",
        "\n",
        "        # Preprocess: Resize and normalize\n",
        "        processed_face = preprocess_image(face, target_size)\n",
        "        images.append(processed_face)\n",
        "        labels.append(target_label)\n",
        "\n",
        "        if (index + 1) % 5000 == 0:\n",
        "            print(f\"Processed {index + 1} / {len(df)} images...\")\n",
        "\n",
        "    print(f\"Finished processing. Skipped {skipped_count} images.\")\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Define the stress mapping\n",
        "# 0=Angry, 2=Fear, 4=Sad -> Stress (1)\n",
        "# 3=Happy, 5=Surprise, 6=Neutral -> NoStress (0)\n",
        "stress_mapping = {0: 1, 2: 1, 4: 1, 3: 0, 5: 0, 6: 0}\n",
        "\n",
        "# Prepare the data (Run this only if fer_df loaded successfully)\n",
        "if fer_df is not None:\n",
        "    X, y = prepare_fer_data(fer_df, IMG_SIZE, stress_mapping, use_face_detection=False) # Turn off face detection for FER as it's already cropped\n",
        "    print(f\"Data shapes: X={X.shape}, y={y.shape}\")\n",
        "\n",
        "    if X.size > 0 and y.size > 0:\n",
        "      # Split data: 70% Train, 15% Validation, 15% Test\n",
        "      X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=SEED, stratify=y)\n",
        "      X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=SEED, stratify=y_temp) # 0.5 * 0.3 = 0.15\n",
        "\n",
        "      print(f\"Train set: X={X_train.shape}, y={y_train.shape}\")\n",
        "      print(f\"Validation set: X={X_val.shape}, y={y_val.shape}\")\n",
        "      print(f\"Test set: X={X_test.shape}, y={y_test.shape}\")\n",
        "\n",
        "      # Check distribution\n",
        "      print(\"\\nTrain label distribution:\", np.bincount(y_train))\n",
        "      print(\"Validation label distribution:\", np.bincount(y_val))\n",
        "      print(\"Test label distribution:\", np.bincount(y_test))\n",
        "\n",
        "      # Create TensorFlow Datasets for efficiency (optional but recommended)\n",
        "      # We can apply augmentations using tf.data or a Keras layer/ImageDataGenerator\n",
        "      # For simplicity with Albumentations, we'll use a custom generator later if needed,\n",
        "      # or apply augmentations directly to the numpy arrays (can be memory intensive).\n",
        "\n",
        "      # Simple Keras ImageDataGenerator for augmentation\n",
        "      train_datagen = ImageDataGenerator(\n",
        "          # Use Albumentations via lambda func or preprocess_input\n",
        "          # preprocessing_function=lambda x: transform_train(image=x)['image'], # Can be slow\n",
        "          # Or use Keras's built-in ones (less variety than Albumentations)\n",
        "          rotation_range=15,\n",
        "          width_shift_range=0.1,\n",
        "          height_shift_range=0.1,\n",
        "          shear_range=0.1,\n",
        "          zoom_range=0.1,\n",
        "          horizontal_flip=True,\n",
        "          fill_mode='nearest'\n",
        "      )\n",
        "      # No augmentation for validation/test data, only rescaling (already done)\n",
        "      val_datagen = ImageDataGenerator() # Rescaling done in preprocess_image\n",
        "\n",
        "      train_generator = train_datagen.flow(X_train, y_train, batch_size=BATCH_SIZE, seed=SEED)\n",
        "      validation_generator = val_datagen.flow(X_val, y_val, batch_size=BATCH_SIZE, seed=SEED)\n",
        "      test_generator = val_datagen.flow(X_test, y_test, batch_size=BATCH_SIZE, shuffle=False) # No shuffle for test evaluation\n",
        "\n",
        "    else:\n",
        "        print(\"Data preparation resulted in empty arrays. Cannot proceed with training.\")\n",
        "        # Create dummy data for code structure continuation\n",
        "        X_train, y_train = np.random.rand(100, IMG_SIZE[0], IMG_SIZE[1], 3), np.random.randint(0, 2, 100)\n",
        "        X_val, y_val = np.random.rand(20, IMG_SIZE[0], IMG_SIZE[1], 3), np.random.randint(0, 2, 20)\n",
        "        X_test, y_test = np.random.rand(20, IMG_SIZE[0], IMG_SIZE[1], 3), np.random.randint(0, 2, 20)\n",
        "        print(\"Using dummy data for demonstration.\")\n",
        "        # Need dummy generators too\n",
        "        train_generator = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "        validation_generator = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "        test_generator = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(BATCH_SIZE) # No shuffle for test evaluation\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"FER DataFrame not loaded. Cannot prepare data.\")\n",
        "    # Create dummy data for code structure continuation\n",
        "    X_train, y_train = np.random.rand(100, IMG_SIZE[0], IMG_SIZE[1], 3), np.random.randint(0, 2, 100)\n",
        "    X_val, y_val = np.random.rand(20, IMG_SIZE[0], IMG_SIZE[1], 3), np.random.randint(0, 2, 20)\n",
        "    X_test, y_test = np.random.rand(20, IMG_SIZE[0], IMG_SIZE[1], 3), np.random.randint(0, 2, 20)\n",
        "    print(\"Using dummy data for demonstration.\")\n",
        "    # Need dummy generators too\n",
        "    train_generator = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "    validation_generator = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "    test_generator = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(BATCH_SIZE) # No shuffle for test evaluation\n",
        "\n",
        "\n",
        "print(\"Data preparation and generator setup complete.\")"
      ],
      "metadata": {
        "id": "hF4qnEJ1NxUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title << 2. Model Development â€“ Facial Stress Detection >>\n",
        "\n",
        "# --- 2.1 Model Definitions ---\n",
        "\n",
        "def build_resnet_gru_model(input_shape, num_classes, gru_units=128, freeze_base=True):\n",
        "    \"\"\"Builds a ResNet50 + GRU model for sequence/temporal analysis (if needed)\n",
        "       or standard classification.\"\"\"\n",
        "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "    if freeze_base:\n",
        "        base_model.trainable = False\n",
        "        print(\"ResNet50 base frozen.\")\n",
        "    else:\n",
        "        # Optionally unfreeze some layers later for fine-tuning\n",
        "        base_model.trainable = True\n",
        "        print(\"ResNet50 base trainable (fine-tuning).\")\n",
        "        # Example: Fine-tune from conv5_block1_out onwards\n",
        "        # fine_tune_at = 143 # Index of 'conv5_block1_out' layer\n",
        "        # for layer in base_model.layers[:fine_tune_at]:\n",
        "        #   layer.trainable = False\n",
        "\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    # Optional: Add TimeDistributed wrapper if input is sequence of images\n",
        "    # x = TimeDistributed(base_model)(inputs)\n",
        "    # x = TimeDistributed(GlobalAveragePooling2D())(x)\n",
        "    # x = GRU(gru_units, return_sequences=False)(x) # Or LSTM\n",
        "\n",
        "    # For single image classification:\n",
        "    x = base_model(inputs, training=not freeze_base) # Use training=False if base is frozen and has BN layers\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    outputs = Dense(num_classes, activation='sigmoid' if num_classes == 1 else 'softmax')(x) # Sigmoid for binary\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    # Compile the model\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4) # Lower LR for fine-tuning if base is unfrozen\n",
        "    loss = 'binary_crossentropy' if num_classes == 1 else 'sparse_categorical_crossentropy'\n",
        "    metrics = ['accuracy',\n",
        "               tf.keras.metrics.AUC(name='roc_auc'),\n",
        "               tf.keras.metrics.Precision(name='precision'),\n",
        "               tf.keras.metrics.Recall(name='recall')]\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "\n",
        "    print(\"ResNet50 + Dense model built.\")\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "def build_efficientnet_model(input_shape, num_classes, base_model_name='B0', freeze_base=True):\n",
        "    \"\"\"Builds an EfficientNet (B0-B7) or MobileNetV3 model.\"\"\"\n",
        "    if 'EfficientNet' in base_model_name:\n",
        "        base_model_cls = getattr(tf.keras.applications, f\"EfficientNet{base_model_name}\")\n",
        "        print(f\"Using EfficientNet{base_model_name}\")\n",
        "    elif 'MobileNetV3' in base_model_name:\n",
        "        base_model_cls = getattr(tf.keras.applications, base_model_name) # e.g., MobileNetV3Small\n",
        "        print(f\"Using {base_model_name}\")\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported base_model_name\")\n",
        "\n",
        "    base_model = base_model_cls(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "    if freeze_base:\n",
        "        base_model.trainable = False\n",
        "        print(f\"{base_model_name} base frozen.\")\n",
        "    else:\n",
        "        base_model.trainable = True\n",
        "        print(f\"{base_model_name} base trainable (fine-tuning).\")\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = base_model(inputs, training=not freeze_base)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    # Rebuild top based on recommendations for EfficientNet/MobileNet\n",
        "    x = Dropout(0.3)(x) # Increased dropout often recommended\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    outputs = Dense(num_classes, activation='sigmoid' if num_classes == 1 else 'softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    # Compile\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3) # Can often start higher for these models\n",
        "    loss = 'binary_crossentropy' if num_classes == 1 else 'sparse_categorical_crossentropy'\n",
        "    metrics = ['accuracy',\n",
        "               tf.keras.metrics.AUC(name='roc_auc'),\n",
        "               tf.keras.metrics.Precision(name='precision'),\n",
        "               tf.keras.metrics.Recall(name='recall')]\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "    print(f\"{base_model_name} + Dense model built.\")\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "\n",
        "# --- 2.2 Model Training ---\n",
        "NUM_CLASSES = 1 # Binary: Stress (1) vs NoStress (0)\n",
        "INPUT_SHAPE = (IMG_SIZE[0], IMG_SIZE[1], 3)\n",
        "\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
        "# Reduce LR on plateau\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6, verbose=1)\n",
        "# Model checkpointing (optional but good practice)\n",
        "resnet_checkpoint_path = str(MODEL_DIR / \"resnet_stress_best.keras\") # Use .keras format\n",
        "efficientnet_checkpoint_path = str(MODEL_DIR / \"efficientnet_stress_best.keras\")\n",
        "\n",
        "resnet_checkpoint = ModelCheckpoint(resnet_checkpoint_path, monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
        "efficientnet_checkpoint = ModelCheckpoint(efficientnet_checkpoint_path, monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
        "\n",
        "\n",
        "# --- Train Model 1: ResNet50 + Dense ---\n",
        "print(\"\\n--- Training ResNet50 Model ---\")\n",
        "# Start with frozen base\n",
        "model_resnet = build_resnet_gru_model(INPUT_SHAPE, NUM_CLASSES, freeze_base=True)\n",
        "\n",
        "# Calculate steps per epoch if using generators\n",
        "steps_per_epoch_train = None\n",
        "validation_steps = None\n",
        "if isinstance(train_generator, tf.keras.preprocessing.image.DirectoryIterator) or isinstance(train_generator, tf.keras.preprocessing.image.NumpyArrayIterator):\n",
        "    steps_per_epoch_train = len(train_generator)\n",
        "    validation_steps = len(validation_generator)\n",
        "elif isinstance(train_generator, tf.data.Dataset):\n",
        "    # For tf.data, steps_per_epoch might not be needed if dataset is not repeating indefinitely\n",
        "    # However, Keras expects it for certain callbacks or progress bars.\n",
        "    # Estimate based on data size / batch size if possible\n",
        "    if hasattr(X_train, 'shape'):\n",
        "        steps_per_epoch_train = int(np.ceil(X_train.shape[0] / BATCH_SIZE))\n",
        "        validation_steps = int(np.ceil(X_val.shape[0] / BATCH_SIZE))\n",
        "\n",
        "\n",
        "# Check if generators are valid before training\n",
        "if train_generator is None or validation_generator is None:\n",
        "    print(\"Error: Data generators are not initialized. Cannot train.\")\n",
        "else:\n",
        "    history_resnet = model_resnet.fit(\n",
        "        train_generator,\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=validation_generator,\n",
        "        steps_per_epoch=steps_per_epoch_train,\n",
        "        validation_steps=validation_steps,\n",
        "        callbacks=[early_stopping, reduce_lr, resnet_checkpoint],\n",
        "        verbose=1\n",
        "    )\n",
        "    print(\"ResNet Training Finished.\")\n",
        "    # Optional: Fine-tuning phase\n",
        "    # model_resnet.trainable = True # Unfreeze base\n",
        "    # ... recompile with lower LR ...\n",
        "    # ... train for few more epochs ...\n",
        "\n",
        "\n",
        "# --- Train Model 2: EfficientNetB0 ---\n",
        "print(\"\\n--- Training EfficientNetB0 Model ---\")\n",
        "model_efficientnet = build_efficientnet_model(INPUT_SHAPE, NUM_CLASSES, base_model_name='B0', freeze_base=True)\n",
        "\n",
        "if train_generator is None or validation_generator is None:\n",
        "     print(\"Error: Data generators are not initialized. Cannot train.\")\n",
        "else:\n",
        "    history_efficientnet = model_efficientnet.fit(\n",
        "        train_generator,\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=validation_generator,\n",
        "        steps_per_epoch=steps_per_epoch_train,\n",
        "        validation_steps=validation_steps,\n",
        "        callbacks=[early_stopping, reduce_lr, efficientnet_checkpoint],\n",
        "        verbose=1\n",
        "    )\n",
        "    print(\"EfficientNet Training Finished.\")\n",
        "\n",
        "\n",
        "# --- 2.3 Model Evaluation ---\n",
        "def evaluate_model(model, model_name, test_data, history):\n",
        "    print(f\"\\n--- Evaluating {model_name} ---\")\n",
        "\n",
        "    # Plot training history\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "    plt.title(f'{model_name} Training History')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    if 'roc_auc' in history.history:\n",
        "      plt.plot(history.history['roc_auc'], label='Train ROC AUC')\n",
        "      plt.plot(history.history['val_roc_auc'], label='Val ROC AUC')\n",
        "      plt.title(f'{model_name} ROC AUC')\n",
        "      plt.xlabel('Epoch')\n",
        "      plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Evaluate on Test Set\n",
        "    print(\"Evaluating on Test Set...\")\n",
        "    if isinstance(test_data, tf.data.Dataset):\n",
        "      results = model.evaluate(test_data, verbose=1)\n",
        "      test_loss = results[0]\n",
        "      test_accuracy = results[1]\n",
        "      test_roc_auc = results[2] # Assuming AUC is the 3rd metric compiled\n",
        "      # Getting F1 requires predictions\n",
        "      y_pred_prob = model.predict(test_data)\n",
        "      y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
        "      # Extracting true labels from tf.data.Dataset is a bit tricky\n",
        "      # It's often easier to evaluate using numpy arrays if possible, or iterate through dataset\n",
        "      y_true = np.concatenate([y for x, y in test_data], axis=0)\n",
        "\n",
        "    elif isinstance(test_data, tf.keras.preprocessing.image.NumpyArrayIterator):\n",
        "        results = model.evaluate(test_data, verbose=1)\n",
        "        test_loss = results[0]\n",
        "        test_accuracy = results[1]\n",
        "        test_roc_auc = results[2] # Assuming AUC is the 3rd metric compiled\n",
        "        y_pred_prob = model.predict(test_data)\n",
        "        y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
        "        y_true = test_data.labels[:len(y_pred)] # Get true labels from generator\n",
        "\n",
        "    else: # Assume test_data is (X_test, y_test) numpy arrays\n",
        "        test_loss, test_accuracy, test_roc_auc, test_precision, test_recall = model.evaluate(test_data[0], test_data[1], verbose=1)\n",
        "        y_pred_prob = model.predict(test_data[0])\n",
        "        y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
        "        y_true = test_data[1]\n",
        "\n",
        "\n",
        "    test_f1 = f1_score(y_true, y_pred)\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "    print(f\"Test ROC AUC: {test_roc_auc:.4f}\")\n",
        "    print(f\"Test F1-Score: {test_f1:.4f}\")\n",
        "\n",
        "    # Classification Report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    # Ensure y_true and y_pred are correctly aligned and sized\n",
        "    print(classification_report(y_true, y_pred, target_names=['NoStress (0)', 'Stress (1)']))\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['NoStress', 'Stress'], yticklabels=['NoStress', 'Stress'])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title(f'{model_name} Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "    return {'loss': test_loss, 'accuracy': test_accuracy, 'roc_auc': test_roc_auc, 'f1': test_f1}\n",
        "\n",
        "\n",
        "# Load best weights before evaluation\n",
        "if os.path.exists(resnet_checkpoint_path):\n",
        "    print(\"Loading best ResNet weights...\")\n",
        "    model_resnet.load_weights(resnet_checkpoint_path)\n",
        "if os.path.exists(efficientnet_checkpoint_path):\n",
        "    print(\"Loading best EfficientNet weights...\")\n",
        "    model_efficientnet.load_weights(efficientnet_checkpoint_path)\n",
        "\n",
        "# Choose the appropriate test data format\n",
        "test_data_eval = (X_test, y_test) if 'X_test' in locals() else test_generator\n",
        "\n",
        "if 'model_resnet' in locals() and 'history_resnet' in locals():\n",
        "  results_resnet = evaluate_model(model_resnet, \"ResNet50\", test_data_eval, history_resnet)\n",
        "else:\n",
        "  print(\"ResNet model or history not available for evaluation.\")\n",
        "\n",
        "if 'model_efficientnet' in locals() and 'history_efficientnet' in locals():\n",
        "  results_efficientnet = evaluate_model(model_efficientnet, \"EfficientNetB0\", test_data_eval, history_efficientnet)\n",
        "else:\n",
        "  print(\"EfficientNet model or history not available for evaluation.\")\n",
        "\n",
        "# --- 2.4 Hyperparameter Optimization (Conceptual) ---\n",
        "# Using Optuna or Ray Tune involves defining an objective function that trains\n",
        "# a model with given hyperparameters and returns a metric (e.g., validation accuracy).\n",
        "# Example structure with Optuna:\n",
        "\"\"\"\n",
        "def objective(trial):\n",
        "    # Suggest hyperparameters\n",
        "    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
        "    dropout_rate = trial.suggest_float('dropout', 0.1, 0.6)\n",
        "    gru_units = trial.suggest_categorical('gru_units', [64, 128, 256])\n",
        "    # ... other hyperparameters ...\n",
        "\n",
        "    # Build model with suggested params\n",
        "    model = build_resnet_gru_model(INPUT_SHAPE, NUM_CLASSES, gru_units=gru_units, ...)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), ...)\n",
        "\n",
        "    # Train model (potentially smaller dataset/epochs for speed)\n",
        "    history = model.fit(...)\n",
        "\n",
        "    # Return the metric to optimize (e.g., max validation accuracy)\n",
        "    return history.history['val_accuracy'][-1]\n",
        "\n",
        "# study = optuna.create_study(direction='maximize')\n",
        "# study.optimize(objective, n_trials=50) # Number of trials\n",
        "# print(\"Best trial:\", study.best_trial.params)\n",
        "\"\"\"\n",
        "print(\"\\nHyperparameter Optimization (Optuna/Ray Tune) conceptualized.\")\n",
        "print(\"Implementation requires defining an objective function and running the study.\")\n",
        "\n",
        "# --- 2.5 Model Export to ONNX and TensorRT Acceleration ---\n",
        "# Ensure tf2onnx is installed: pip install tf2onnx\n",
        "# ONNX Runtime (CPU or GPU): pip install onnxruntime or onnxruntime-gpu\n",
        "import tf2onnx\n",
        "import onnxruntime as ort\n",
        "\n",
        "def export_to_onnx(model, model_name, output_dir):\n",
        "    \"\"\"Exports a Keras model to ONNX format.\"\"\"\n",
        "    onnx_model_path = str(output_dir / f\"{model_name}.onnx\")\n",
        "    print(f\"Exporting {model_name} to ONNX: {onnx_model_path}\")\n",
        "\n",
        "    try:\n",
        "        # Convert the model\n",
        "        # spec = (tf.TensorSpec(model.input.shape, model.input.dtype, name=\"input\"),) # Fails with generator sometimes\n",
        "        # Use concrete function\n",
        "        # Need to define input signature explicitly\n",
        "        input_signature = [tf.TensorSpec([None, INPUT_SHAPE[0], INPUT_SHAPE[1], INPUT_SHAPE[2]], tf.float32, name='input_image')]\n",
        "\n",
        "        model_proto, external_tensor_storage = tf2onnx.convert.from_keras(\n",
        "            model, input_signature=input_signature, opset=13, output_path=onnx_model_path\n",
        "        )\n",
        "        print(f\"Model successfully converted to ONNX format at {onnx_model_path}\")\n",
        "\n",
        "        # Optional: Verify the ONNX model\n",
        "        print(\"Verifying ONNX model...\")\n",
        "        ort_session = ort.InferenceSession(onnx_model_path, providers=['CUDAExecutionProvider', 'CPUExecutionProvider']) # Use GPU if available\n",
        "        print(f\"ONNX model loaded successfully with providers: {ort_session.get_providers()}\")\n",
        "        input_name = ort_session.get_inputs()[0].name\n",
        "        output_name = ort_session.get_outputs()[0].name\n",
        "        print(f\"Input name: {input_name}, Output name: {output_name}\")\n",
        "\n",
        "        # Test with a dummy input\n",
        "        if 'X_test' in locals() and X_test.shape[0] > 0:\n",
        "           dummy_input = X_test[0:1] # Batch size of 1\n",
        "        else:\n",
        "           dummy_input = np.random.rand(1, INPUT_SHAPE[0], INPUT_SHAPE[1], INPUT_SHAPE[2]).astype(np.float32)\n",
        "\n",
        "        ort_inputs = {input_name: dummy_input}\n",
        "        ort_outs = ort_session.run([output_name], ort_inputs)\n",
        "        print(f\"ONNX model prediction (test): {ort_outs[0]}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during ONNX conversion or verification for {model_name}: {e}\")\n",
        "        onnx_model_path = None # Indicate failure\n",
        "\n",
        "    return onnx_model_path\n",
        "\n",
        "# Select the best performing model based on evaluation metrics\n",
        "# For demo, let's assume EfficientNet performed better or is preferred for mobile\n",
        "# In a real scenario, compare results_resnet and results_efficientnet\n",
        "chosen_model = model_efficientnet # Example choice\n",
        "chosen_model_name = \"efficientnet_stress_model\"\n",
        "# Or choose based on metrics:\n",
        "# best_model_name = \"ResNet50\" if results_resnet['accuracy'] > results_efficientnet['accuracy'] else \"EfficientNetB0\"\n",
        "\n",
        "if 'chosen_model' in locals():\n",
        "    onnx_path = export_to_onnx(chosen_model, chosen_model_name, ONNX_MODEL_DIR)\n",
        "else:\n",
        "    print(\"No model was successfully trained or selected for ONNX export.\")\n",
        "    onnx_path = None\n",
        "\n",
        "# TensorRT Acceleration (Conceptual - Requires NVIDIA GPU, CUDA, cuDNN, TensorRT installed)\n",
        "# Typically done during deployment on the target NVIDIA hardware.\n",
        "# Process:\n",
        "# 1. Build a TensorRT engine from the ONNX model using `trtexec` command-line tool or TensorRT Python API.\n",
        "#    `trtexec --onnx=model.onnx --saveEngine=model.trt --fp16` (Example command)\n",
        "# 2. Use NVIDIA Triton Inference Server or custom C++/Python code with the TensorRT runtime to load the .trt engine for inference.\n",
        "print(\"\\nTensorRT Acceleration:\")\n",
        "print(\"1. Export the ONNX model (done above).\")\n",
        "print(\"2. Use `trtexec` or TensorRT API on target NVIDIA hardware to build a .trt engine.\")\n",
        "print(\"   Example: trtexec --onnx={onnx_path} --saveEngine={ONNX_MODEL_DIR / (chosen_model_name + '.trt')} --fp16\")\n",
        "print(\"3. Deploy the .trt engine using Triton Inference Server or custom TensorRT runtime application.\")\n",
        "\n",
        "# Final Trained Models (Keras format)\n",
        "final_resnet_path = str(MODEL_DIR / \"resnet_stress_final.keras\")\n",
        "final_efficientnet_path = str(MODEL_DIR / \"efficientnet_stress_final.keras\")\n",
        "\n",
        "if 'model_resnet' in locals(): model_resnet.save(final_resnet_path)\n",
        "if 'model_efficientnet' in locals(): model_efficientnet.save(final_efficientnet_path)\n",
        "print(f\"Final Keras models saved to {MODEL_DIR}\")"
      ],
      "metadata": {
        "id": "c8Io0hoCN1Tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title << 3. Model Development â€“ Shift Optimization >>\n",
        "\n",
        "# --- 3.1 Shift Optimization Engine using Google OR-Tools (CP-SAT) ---\n",
        "\n",
        "# Define Inputs (Example Data)\n",
        "num_doctors = 5\n",
        "num_days = 7\n",
        "num_shifts = 3 # e.g., Morning (0), Afternoon (1), Night (2)\n",
        "\n",
        "# Doctor availability (doctor, day): 1 if available, 0 otherwise\n",
        "# Could be more complex (e.g., preferred days off)\n",
        "availability = np.random.randint(0, 2, size=(num_doctors, num_days))\n",
        "# For demo, assume everyone is available every day initially\n",
        "availability = np.ones((num_doctors, num_days), dtype=int)\n",
        "\n",
        "# Doctor skills (doctor, skill): 1 if possesses skill\n",
        "# Example skills: 0=General, 1=Surgery, 2=Cardiology\n",
        "num_skills = 3\n",
        "skills = np.random.randint(0, 2, size=(num_doctors, num_skills))\n",
        "# Ensure at least one doctor per skill for feasibility\n",
        "skills[0, 1] = 1 # Doctor 0 has Surgery skill\n",
        "skills[1, 2] = 1 # Doctor 1 has Cardiology skill\n",
        "skills[:, 0] = 1 # All doctors have General skill\n",
        "\n",
        "# Shift requirements (day, shift, skill): Min number of doctors needed\n",
        "# Making this up for the example\n",
        "shift_skill_requirements = np.zeros((num_days, num_shifts, num_skills), dtype=int)\n",
        "shift_skill_requirements[:, :, 0] = 1 # Need 1 general doctor per shift\n",
        "shift_skill_requirements[:, 0, 1] = 1 # Need 1 surgeon in the morning shift\n",
        "shift_skill_requirements[:, 1, 2] = 1 # Need 1 cardiologist in the afternoon\n",
        "\n",
        "# Workload Logs (Conceptual - used to estimate future stress or load)\n",
        "# Example: workload[doctor, day] = hours worked or complexity score\n",
        "workload_logs = np.random.uniform(6, 10, size=(num_doctors, num_days))\n",
        "\n",
        "# Stress Levels (Input from the Facial Stress Detection Model)\n",
        "# Example: stress_levels[doctor, day] = predicted average stress score (0-1)\n",
        "# This would be updated dynamically based on real-time predictions\n",
        "current_stress_levels = np.random.uniform(0.1, 0.8, size=num_doctors) # Current stress for each doctor\n",
        "\n",
        "# Constraints (Legal, Policy, Preferences)\n",
        "max_shifts_per_week = 5\n",
        "min_shifts_per_week = 2 # Ensure fairness\n",
        "max_consecutive_shifts = 2\n",
        "min_rest_between_shifts = 2 # E.g., must have 2 shifts off (16 hours if shifts are 8h) after a shift\n",
        "max_stress_threshold = 0.7 # Doctors exceeding this should ideally get lighter load or rest\n",
        "\n",
        "# --- CP-SAT Model ---\n",
        "model = cp_model.CpModel()\n",
        "\n",
        "# Decision Variables: shifts[(d, day, s)] = 1 if doctor d works shift s on day, 0 otherwise\n",
        "shifts = {}\n",
        "for d in range(num_doctors):\n",
        "    for day in range(num_days):\n",
        "        for s in range(num_shifts):\n",
        "            shifts[(d, day, s)] = model.NewBoolVar(f'shift_d{d}_day{day}_s{s}')\n",
        "\n",
        "# --- Constraints ---\n",
        "\n",
        "# 1. Availability: Doctor cannot work if unavailable (if availability matrix is used)\n",
        "# for d in range(num_doctors):\n",
        "#     for day in range(num_days):\n",
        "#         if availability[d, day] == 0:\n",
        "#             for s in range(num_shifts):\n",
        "#                 model.Add(shifts[(d, day, s)] == 0)\n",
        "\n",
        "# 2. Shift Skill Requirements: Ensure enough skilled doctors are assigned\n",
        "for day in range(num_days):\n",
        "    for s in range(num_shifts):\n",
        "        for skill in range(num_skills):\n",
        "            required_count = shift_skill_requirements[day, s, skill]\n",
        "            if required_count > 0:\n",
        "                model.Add(sum(shifts[(d, day, s)] * skills[d, skill] for d in range(num_doctors)) >= required_count)\n",
        "\n",
        "# 3. One Shift Per Doctor Per Day (at most): A doctor can only work one shift on a given day\n",
        "for d in range(num_doctors):\n",
        "    for day in range(num_days):\n",
        "        model.Add(sum(shifts[(d, day, s)] for s in range(num_shifts)) <= 1)\n",
        "\n",
        "# 4. Weekly Workload Limits: Min/Max number of shifts per doctor per week\n",
        "for d in range(num_doctors):\n",
        "    total_shifts_worked = sum(shifts[(d, day, s)] for day in range(num_days) for s in range(num_shifts))\n",
        "    model.Add(total_shifts_worked >= min_shifts_per_week)\n",
        "    model.Add(total_shifts_worked <= max_shifts_per_week)\n",
        "\n",
        "# 5. Max Consecutive Shifts: (Slightly more complex constraint)\n",
        "# for d in range(num_doctors):\n",
        "#     for day in range(num_days - max_consecutive_shifts):\n",
        "#         for s in range(num_shifts):\n",
        "#             # This needs careful indexing if shifts wrap around days/nights\n",
        "#             # Simplified: Check within a day first\n",
        "#             # TODO: Implement robust consecutive shift constraint (might need helper variables)\n",
        "            pass # Placeholder for brevity\n",
        "\n",
        "\n",
        "# 6. Minimum Rest Between Shifts: (Also complex, depends on shift timing)\n",
        "# Similar to consecutive shifts, requires careful indexing across days.\n",
        "# Example: If doctor works day 'd' shift 's', cannot work day 'd' shift 's+1', 's+2' (if < min_rest)\n",
        "# and potentially day 'd+1' shift 0 etc.\n",
        "# TODO: Implement robust min rest constraint\n",
        "\n",
        "# 7. Stress Constraint (Example): Doctors with high stress should not work night shifts (shift 2)\n",
        "for d in range(num_doctors):\n",
        "    if current_stress_levels[d] > max_stress_threshold:\n",
        "        print(f\"Applying stress constraint for Doctor {d} (Stress: {current_stress_levels[d]:.2f})\")\n",
        "        for day in range(num_days):\n",
        "            model.Add(shifts[(d, day, 2)] == 0) # Cannot work night shift (index 2)\n",
        "\n",
        "# --- Objective Function ---\n",
        "# Example Objective: Minimize total number of shifts assigned (implies efficiency)\n",
        "# OR: Maximize fairness (e.g., minimize variance in shifts worked)\n",
        "# OR: Minimize assignments for high-stress doctors (weighted)\n",
        "\n",
        "# Objective: Minimize sum of (stress_level * shift_assignment)\n",
        "# This encourages assigning fewer shifts to stressed doctors. Use quadratic term? or linear approx\n",
        "objective_terms = []\n",
        "for d in range(num_doctors):\n",
        "    for day in range(num_days):\n",
        "        for s in range(num_shifts):\n",
        "            # Weight shift assignment by doctor's current stress level\n",
        "            # Scale stress to make it a meaningful cost\n",
        "            stress_cost = int(current_stress_levels[d] * 100) # Scale 0-1 stress to 0-100 cost\n",
        "            objective_terms.append(shifts[(d, day, s)] * stress_cost)\n",
        "\n",
        "model.Minimize(sum(objective_terms))\n",
        "\n",
        "# --- Solve ---\n",
        "solver = cp_model.CpSolver()\n",
        "solver.parameters.max_time_in_seconds = 60.0 # Set time limit\n",
        "solver.parameters.log_search_progress = True\n",
        "status = solver.Solve(model)\n",
        "\n",
        "# --- Process Results ---\n",
        "if status == cp_model.OPTIMAL or status == cp_model.FEASIBLE:\n",
        "    print(f'\\nSolution found (Status: {solver.StatusName(status)})')\n",
        "    print(f'Objective value (Stress-weighted cost): {solver.ObjectiveValue()}')\n",
        "\n",
        "    schedule = np.zeros((num_doctors, num_days, num_shifts), dtype=int)\n",
        "    total_shifts = np.zeros(num_doctors, dtype=int)\n",
        "\n",
        "    print(\"\\nGenerated Schedule:\")\n",
        "    header = \"Doctor | \" + \" | \".join([f\"Day {day}\" for day in range(num_days)])\n",
        "    print(header)\n",
        "    print(\"-\" * len(header))\n",
        "\n",
        "    for d in range(num_doctors):\n",
        "        row = f\"  {d: <4} | \"\n",
        "        for day in range(num_days):\n",
        "            shift_str = \" --- \"\n",
        "            for s in range(num_shifts):\n",
        "                if solver.Value(shifts[(d, day, s)]) == 1:\n",
        "                    schedule[d, day, s] = 1\n",
        "                    total_shifts[d] += 1\n",
        "                    shift_str = f\"  S{s} \" # Indicate Shift 0, 1, or 2\n",
        "                    break # Assumes max 1 shift per day constraint holds\n",
        "            row += shift_str + \" | \"\n",
        "        row += f\" (Total: {total_shifts[d]}, Stress: {current_stress_levels[d]:.2f})\"\n",
        "        print(row)\n",
        "\n",
        "    print(\"\\nVerifying Skill Requirements (Example Day 0):\")\n",
        "    day_check = 0\n",
        "    for s in range(num_shifts):\n",
        "        for skill in range(num_skills):\n",
        "            required = shift_skill_requirements[day_check, s, skill]\n",
        "            if required > 0:\n",
        "                assigned_count = sum(solver.Value(shifts[(d, day_check, s)]) * skills[d, skill] for d in range(num_doctors))\n",
        "                print(f\"Day {day_check}, Shift {s}, Skill {skill}: Required={required}, Assigned={assigned_count} {'(OK)' if assigned_count >= required else '(PROBLEM!)'}\")\n",
        "\n",
        "elif status == cp_model.INFEASIBLE:\n",
        "    print('\\nSolver returned INFEASIBLE. Constraints cannot be satisfied.')\n",
        "    print(\"Check constraints, requirements, and availability.\")\n",
        "    # TODO: Implement fallback logic or constraint relaxation here\n",
        "elif status == cp_model.MODEL_INVALID:\n",
        "    print('\\nSolver returned MODEL_INVALID. Check model definition.')\n",
        "else:\n",
        "    print(f'\\nSolver returned status: {solver.StatusName(status)}')\n",
        "\n",
        "\n",
        "# --- 3.2 Reinforcement Learning Agent (Conceptual) ---\n",
        "# RL for adaptive scheduling is complex.\n",
        "# - State: Current schedule, predicted stress levels, workload backlog, doctor availability/preferences.\n",
        "# - Action: Swap shifts, assign open shifts, modify schedule based on rules/predictions.\n",
        "# - Reward: Negative of the CP-SAT objective (e.g., -total_stress_cost), penalties for constraint violations, bonuses for smooth operations.\n",
        "# - Algorithm: Actor-Critic (e.g., PPO, A2C) or DQN if action space is discrete and manageable.\n",
        "# - Training: Requires a simulator environment that models hospital operations and stress accumulation. This is non-trivial to build.\n",
        "print(\"\\nReinforcement Learning for Shift Optimization (Conceptual):\")\n",
        "print(\"- State: Schedule, Stress Levels, Workload, Availability\")\n",
        "print(\"- Action: Swap/Assign Shifts\")\n",
        "print(\"- Reward: -(Stress Cost), Constraint Penalties\")\n",
        "print(\"- Requires a simulation environment for training.\")\n",
        "\n",
        "# --- 3.3 Fallback Rule-Based Logic ---\n",
        "def apply_fallback_rules(schedule, stress_levels, threshold):\n",
        "    \"\"\"Simple rule-based adjustments for safety.\"\"\"\n",
        "    new_schedule = schedule.copy()\n",
        "    print(\"\\nApplying Fallback Rules...\")\n",
        "    for d in range(num_doctors):\n",
        "        if stress_levels[d] > threshold:\n",
        "            # Rule: If highly stressed, try to remove demanding shifts (e.g., night shifts)\n",
        "            # This example just prints a warning, but could modify 'new_schedule'\n",
        "            if np.sum(new_schedule[d, :, 2]) > 0: # Check if assigned any night shifts\n",
        "                 print(f\"Fallback Rule Warning: Doctor {d} has high stress ({stress_levels[d]:.2f}) and is assigned night shifts.\")\n",
        "                 # In reality: Attempt to find a swap or remove the shift, potentially triggering re-optimization\n",
        "    print(\"Fallback rule check complete.\")\n",
        "    return new_schedule\n",
        "\n",
        "if 'schedule' in locals():\n",
        "    final_schedule = apply_fallback_rules(schedule, current_stress_levels, STRESS_THRESHOLD)\n",
        "else:\n",
        "    print(\"No initial schedule generated, skipping fallback rules.\")\n",
        "\n",
        "\n",
        "# --- 3.4 Synthetic Training Data (NSPLib etc.) ---\n",
        "# For training RL or evaluating different optimization strategies, benchmark datasets are useful.\n",
        "# - NSPLib (Nurse Scheduling Problem Library): Standard benchmark for nurse rostering.\n",
        "# - Kaggle datasets related to employee scheduling or rostering problems.\n",
        "# These often provide realistic constraints and instance sizes.\n",
        "print(\"\\nSynthetic Data Generation:\")\n",
        "print(\"- Use benchmarks like NSPLib for realistic scheduling problems.\")\n",
        "print(\"- Generate data by varying num_doctors, num_days, constraints, skill mixes.\")"
      ],
      "metadata": {
        "id": "iOSJZWt0N8MW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title << 4. System Architecture & Microservices >>\n",
        "\n",
        "# --- 4.1 System Flowchart (Mermaid) ---\n",
        "# Using Mermaid syntax within a markdown cell for visualization\n",
        "\n",
        "\"\"\"\n",
        "%%mermaid\n",
        "graph LR\n",
        "    subgraph Edge Device (e.g., Camera at Workstation)\n",
        "        A[Camera Input] --> B(Face Detection / Preprocessing);\n",
        "        B --> C{Feature Encoding};\n",
        "        C --> D[Send Features/Cropped Face];\n",
        "    end\n",
        "\n",
        "    subgraph Cloud Backend\n",
        "        D --> E(API Gateway / Load Balancer);\n",
        "        E --> F[FastAPI Inference Service];\n",
        "        F -- Request Features --> C;\n",
        "        F -- Image/Features --> G((Stress Detection Model ONNX/TRT));\n",
        "        G -- Stress Score --> F;\n",
        "        F -- Store Stress Log --> H{Time-Series Aggregator};\n",
        "        H -- Aggregated Stress --> I[SQL Server Database];\n",
        "        H -- Trigger? --> J(FastAPI Shift Optimizer Service);\n",
        "        J -- Request Data (Stress, Workload) --> I;\n",
        "        J -- Request Constraints --> K(Configuration/Rules DB);\n",
        "        J --> L((OR-Tools/RL Engine));\n",
        "        L -- Optimized Schedule --> J;\n",
        "        J -- Store Schedule --> I;\n",
        "    end\n",
        "\n",
        "    subgraph Frontend / Users\n",
        "        M(Flutter Mobile/Web App) --> E;\n",
        "        M -- View Dashboards --> I;\n",
        "        M -- Receive Alerts (WebSocket/Polling) --> F;\n",
        "        M -- Request Manual Optimization --> J;\n",
        "        N(Admin Interface) --> E;\n",
        "        N -- Manage Settings --> K;\n",
        "        N -- View Reports --> I;\n",
        "    end\n",
        "\n",
        "    %% Styling (Optional)\n",
        "    classDef edge fill:#f9f,stroke:#333,stroke-width:2px;\n",
        "    classDef cloud fill:#ccf,stroke:#333,stroke-width:2px;\n",
        "    classDef user fill:#cfc,stroke:#333,stroke-width:2px;\n",
        "    class A,B,C,D edge;\n",
        "    class E,F,G,H,I,J,K,L cloud;\n",
        "    class M,N user;\n",
        "\"\"\"\n",
        "\n",
        "# --- 4.2 Database Schema (SQLAlchemy - Conceptual) ---\n",
        "\n",
        "DB_URL = \"sqlite:///hospital_schedule.db\" # Example using SQLite for demo; replace with SQL Server connection string\n",
        "# SQL Server example: \"mssql+pyodbc://user:password@server/database?driver=ODBC+Driver+17+for+SQL+Server\"\n",
        "engine = create_engine(DB_URL)\n",
        "metadata = MetaData()\n",
        "\n",
        "# Doctors Table\n",
        "doctors_table = Table('doctors', metadata,\n",
        "    Column('doctor_id', Integer, primary_key=True),\n",
        "    Column('name', String(100), nullable=False),\n",
        "    Column('email', String(100), unique=True), # For notifications/login\n",
        "    Column('specialty', String(100)),\n",
        "    Column('current_avg_stress', Float, default=0.0), # Updated periodically\n",
        "    Column('created_at', DateTime, default=datetime.datetime.utcnow)\n",
        ")\n",
        "\n",
        "# Skills Table (if needed for complex skill management)\n",
        "# skills_table = Table('skills', metadata, ...)\n",
        "# doctor_skills_table = Table('doctor_skills', metadata, ...) # Many-to-many mapping\n",
        "\n",
        "# Stress Logs Table\n",
        "stress_logs_table = Table('stress_logs', metadata,\n",
        "    Column('log_id', Integer, primary_key=True),\n",
        "    Column('doctor_id', Integer, ForeignKey('doctors.doctor_id'), nullable=False),\n",
        "    Column('timestamp', DateTime, default=datetime.datetime.utcnow, index=True),\n",
        "    Column('raw_stress_score', Float, nullable=False), # Output from the model\n",
        "    Column('smoothed_stress_score', Float), # Optional: after temporal smoothing\n",
        "    Column('image_ref', String(255)), # Optional reference to anonymized image/features for audit (use with caution!)\n",
        "    Column('source_device', String(50)) # e.g., 'Workstation-1A'\n",
        ")\n",
        "\n",
        "# Schedules Table\n",
        "schedules_table = Table('schedules', metadata,\n",
        "    Column('schedule_id', Integer, primary_key=True),\n",
        "    Column('doctor_id', Integer, ForeignKey('doctors.doctor_id'), nullable=False),\n",
        "    Column('schedule_date', DateTime, nullable=False, index=True),\n",
        "    Column('shift_type', Integer, nullable=False), # 0=Morning, 1=Afternoon, 2=Night\n",
        "    Column('assigned_at', DateTime, default=datetime.datetime.utcnow),\n",
        "    Column('schedule_version', Integer, default=1) # To track optimization runs\n",
        ")\n",
        "\n",
        "# Workload Logs Table (Example)\n",
        "workload_logs_table = Table('workload_logs', metadata,\n",
        "     Column('log_id', Integer, primary_key=True),\n",
        "     Column('doctor_id', Integer, ForeignKey('doctors.doctor_id'), nullable=False),\n",
        "     Column('log_date', DateTime, nullable=False),\n",
        "     Column('hours_worked', Float),\n",
        "     Column('tasks_completed', Integer),\n",
        "     # Other relevant workload metrics\n",
        ")\n",
        "\n",
        "\n",
        "# Create tables in the database\n",
        "print(\"\\nCreating database schema (if it doesn't exist)...\")\n",
        "metadata.create_all(engine)\n",
        "print(\"Database schema defined.\")\n",
        "\n",
        "# --- 4.3 FastAPI Microservice (Inference & Basic Scheduling Trigger) ---\n",
        "\n",
        "app = fastapi.FastAPI(title=\"Doctor Stress & Schedule API\")\n",
        "\n",
        "# --- Pydantic Models for API Requests/Responses ---\n",
        "class ImageInput(BaseModel):\n",
        "    image_b64: str # Base64 encoded image string\n",
        "    doctor_id: int\n",
        "    source_device: str | None = None\n",
        "\n",
        "class StressPredictionResponse(BaseModel):\n",
        "    doctor_id: int\n",
        "    stress_probability: float\n",
        "    is_stressed: bool\n",
        "    timestamp: datetime.datetime\n",
        "    message: str\n",
        "\n",
        "class OptimizeScheduleRequest(BaseModel):\n",
        "    target_date: datetime.date\n",
        "    force_run: bool = False # Option to force re-optimization\n",
        "\n",
        "class OptimizeScheduleResponse(BaseModel):\n",
        "    status: str\n",
        "    message: str\n",
        "    schedule_version: int | None = None # Version ID of the generated schedule\n",
        "\n",
        "# --- Load ONNX Model for Inference ---\n",
        "onnx_model = None\n",
        "onnx_session = None\n",
        "onnx_input_name = None\n",
        "onnx_output_name = None\n",
        "\n",
        "if onnx_path and os.path.exists(onnx_path):\n",
        "    try:\n",
        "        onnx_session = ort.InferenceSession(onnx_path, providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
        "        onnx_input_name = onnx_session.get_inputs()[0].name\n",
        "        onnx_output_name = onnx_session.get_outputs()[0].name\n",
        "        print(f\"ONNX stress detection model loaded successfully for API. Input: {onnx_input_name}, Output: {onnx_output_name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading ONNX model for API: {e}\")\n",
        "        onnx_session = None\n",
        "else:\n",
        "    print(\"ONNX model file not found or not generated. Inference endpoint will be disabled.\")\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def decode_image(b64_string):\n",
        "    image_bytes = b64decode(b64_string)\n",
        "    image = cv2.imdecode(np.frombuffer(image_bytes, np.uint8), cv2.IMREAD_COLOR)\n",
        "    return image\n",
        "\n",
        "def preprocess_for_onnx(image, target_size):\n",
        "    # Perform face detection if needed (or assume input is already cropped)\n",
        "    face_img, _ = detect_face(image) # Use BGR image\n",
        "    if face_img is None or face_img.size == 0:\n",
        "        print(\"No face detected in API input.\")\n",
        "        # Option 1: Return error\n",
        "        # raise ValueError(\"No face detected\")\n",
        "        # Option 2: Try processing the whole image (might be inaccurate)\n",
        "        face_img = image\n",
        "        # Option 3: Return a default \"low confidence\" prediction\n",
        "        # return None\n",
        "\n",
        "    # Resize and normalize (same as training)\n",
        "    processed_face = preprocess_image(face_img, target_size) # Returns float32 [0,1]\n",
        "\n",
        "    # Add batch dimension and ensure correct type\n",
        "    input_tensor = np.expand_dims(processed_face, axis=0).astype(np.float32)\n",
        "    return input_tensor\n",
        "\n",
        "\n",
        "# --- API Endpoints ---\n",
        "@app.get(\"/\")\n",
        "async def read_root():\n",
        "    return {\"message\": \"Welcome to the Doctor Stress Detection and Shift Optimization API\"}\n",
        "\n",
        "@app.post(\"/predict_stress\", response_model=StressPredictionResponse)\n",
        "async def predict_stress(input_data: ImageInput):\n",
        "    \"\"\"Receives Base64 encoded image, performs inference, returns stress probability.\"\"\"\n",
        "    if onnx_session is None or onnx_input_name is None:\n",
        "        raise fastapi.HTTPException(status_code=503, detail=\"Stress detection model not loaded.\")\n",
        "\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        image_bgr = decode_image(input_data.image_b64)\n",
        "\n",
        "        if image_bgr is None or image_bgr.size == 0:\n",
        "             raise fastapi.HTTPException(status_code=400, detail=\"Invalid image data.\")\n",
        "\n",
        "        # Preprocess image for the ONNX model\n",
        "        input_tensor = preprocess_for_onnx(image_bgr, IMG_SIZE) # IMG_SIZE defined earlier\n",
        "        if input_tensor is None:\n",
        "             # Handle case where preprocessing failed (e.g., no face detected)\n",
        "             return StressPredictionResponse(\n",
        "                 doctor_id=input_data.doctor_id,\n",
        "                 stress_probability=-1.0, # Indicate failure/low confidence\n",
        "                 is_stressed=False,\n",
        "                 timestamp=datetime.datetime.utcnow(),\n",
        "                 message=\"Prediction failed (e.g., face not detected).\"\n",
        "             )\n",
        "\n",
        "\n",
        "        # Run inference\n",
        "        ort_inputs = {onnx_input_name: input_tensor}\n",
        "        ort_outs = onnx_session.run([onnx_output_name], ort_inputs)\n",
        "        stress_prob = float(ort_outs[0][0][0]) # Output is likely [[prob]]\n",
        "\n",
        "        is_stressed = stress_prob > STRESS_THRESHOLD\n",
        "        end_time = time.time()\n",
        "        processing_time = end_time - start_time\n",
        "\n",
        "        # --- Store stress log in DB (Asynchronous task recommended for production) ---\n",
        "        try:\n",
        "            conn = engine.connect()\n",
        "            ins = stress_logs_table.insert().values(\n",
        "                doctor_id=input_data.doctor_id,\n",
        "                timestamp=datetime.datetime.utcnow(),\n",
        "                raw_stress_score=stress_prob,\n",
        "                source_device=input_data.source_device\n",
        "            )\n",
        "            conn.execute(ins)\n",
        "            conn.commit() # Use commit method on Connection object\n",
        "            conn.close() # Close connection\n",
        "            db_log_msg = \"Stress log saved.\"\n",
        "        except Exception as db_err:\n",
        "            print(f\"Database Error logging stress: {db_err}\")\n",
        "            db_log_msg = \"Failed to save stress log.\"\n",
        "            # Consider retry logic or logging to a file as fallback\n",
        "\n",
        "        return StressPredictionResponse(\n",
        "            doctor_id=input_data.doctor_id,\n",
        "            stress_probability=stress_prob,\n",
        "            is_stressed=is_stressed,\n",
        "            timestamp=datetime.datetime.utcnow(),\n",
        "            message=f\"Prediction successful ({processing_time:.3f}s). {db_log_msg}\"\n",
        "        )\n",
        "\n",
        "    except ValueError as ve: # Catch specific errors like no face detected\n",
        "        raise fastapi.HTTPException(status_code=400, detail=str(ve))\n",
        "    except Exception as e:\n",
        "        print(f\"Error during prediction: {e}\")\n",
        "        raise fastapi.HTTPException(status_code=500, detail=f\"Internal server error during prediction: {e}\")\n",
        "\n",
        "\n",
        "@app.post(\"/optimize_schedule\", response_model=OptimizeScheduleResponse)\n",
        "async def trigger_schedule_optimization(request: OptimizeScheduleRequest):\n",
        "    \"\"\"Triggers the shift optimization process (placeholder).\"\"\"\n",
        "    print(f\"Received request to optimize schedule for date: {request.target_date}\")\n",
        "    # --- In a real system: ---\n",
        "    # 1. Check if optimization is already running or recently completed.\n",
        "    # 2. Fetch required data (current stress levels, workload, availability) from the database.\n",
        "    # 3. Fetch constraints from config or DB.\n",
        "    # 4. Run the OR-Tools solver (potentially as a background task using Celery, RQ, or FastAPI's BackgroundTasks).\n",
        "    # 5. Store the new schedule in the database with a new version number.\n",
        "    # 6. Handle infeasible solutions (alert admin, use fallback).\n",
        "\n",
        "    # --- Placeholder Implementation ---\n",
        "    # Simulate running the optimization logic defined earlier\n",
        "    # You would fetch real data here instead of using the example globals\n",
        "    print(\"Simulating optimization run...\")\n",
        "    # Re-run the OR-Tools part (or call a dedicated function)\n",
        "    # This is synchronous here, make async in production\n",
        "    try:\n",
        "        # --- Fetch current average stress from DB (Example) ---\n",
        "        # This part needs refinement - should likely average recent stress logs\n",
        "        avg_stress_data = {}\n",
        "        conn = engine.connect()\n",
        "        select_stmt = db.select(doctors_table.c.doctor_id, doctors_table.c.current_avg_stress)\n",
        "        results = conn.execute(select_stmt)\n",
        "        for row in results:\n",
        "             # Use tuple access if using older SQLAlchemy or check row mapping\n",
        "             try:\n",
        "                 avg_stress_data[row[0]] = row[1] # Assuming column order\n",
        "             except AttributeError: # If row is RowProxy or similar\n",
        "                 avg_stress_data[row.doctor_id] = row.current_avg_stress\n",
        "\n",
        "        conn.close()\n",
        "        # Update the `current_stress_levels` used by the solver\n",
        "        simulated_current_stress = np.array([avg_stress_data.get(d, 0.3) for d in range(num_doctors)]) # Default stress if not found\n",
        "        print(\"Fetched/Simulated Stress Levels for Optimization:\", simulated_current_stress)\n",
        "\n",
        "        # --- Run OR-Tools Solver (using fetched data) ---\n",
        "        # This would ideally be a function call:\n",
        "        # success, objective_value, final_schedule, schedule_version = run_or_tools_solver(simulated_current_stress, ...)\n",
        "        # For demo, we just print success message\n",
        "        success = True # Assume it worked for the demo\n",
        "        simulated_version = random.randint(100, 999)\n",
        "\n",
        "        if success:\n",
        "            # --- Store the generated schedule in the DB ---\n",
        "            # This needs the actual 'final_schedule' array from solver\n",
        "            # Loop through final_schedule and insert into 'schedules_table'\n",
        "            # ... (DB insertion logic) ...\n",
        "            print(\"Placeholder: Schedule would be saved to DB here.\")\n",
        "\n",
        "            return OptimizeScheduleResponse(\n",
        "                status=\"Success\",\n",
        "                message=f\"Shift optimization completed successfully for {request.target_date}. Schedule Version: {simulated_version}\",\n",
        "                schedule_version=simulated_version\n",
        "            )\n",
        "        else:\n",
        "             return OptimizeScheduleResponse(\n",
        "                status=\"Failed\",\n",
        "                message=f\"Shift optimization failed or was infeasible for {request.target_date}.\",\n",
        "                schedule_version=None\n",
        "            )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during schedule optimization trigger: {e}\")\n",
        "        raise fastapi.HTTPException(status_code=500, detail=f\"Internal server error during optimization: {e}\")\n",
        "\n",
        "\n",
        "# --- Run FastAPI app using Uvicorn and ngrok (for Colab) ---\n",
        "# Function to run FastAPI in a separate thread and expose via ngrok\n",
        "def run_fastapi():\n",
        "    nest_asyncio.apply() # Allow running uvicorn in Colab's event loop\n",
        "    # Set ngrok authtoken (replace with your token if needed, get from ngrok dashboard)\n",
        "    # ngrok.set_auth_token(\"YOUR_NGROK_AUTHTOKEN\") # Optional, usually needed for more features/longer sessions\n",
        "    port = 8000\n",
        "    # Kill existing ngrok tunnels if any\n",
        "    ngrok.kill()\n",
        "    # Open ngrok tunnel\n",
        "    public_url = ngrok.connect(port)\n",
        "    print(f\"FastAPI running on: {public_url}\")\n",
        "    # Start uvicorn server\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=port, log_level=\"info\")\n",
        "\n",
        "# Start FastAPI in a background thread so Colab notebook execution can continue\n",
        "print(\"\\nStarting FastAPI server in background thread...\")\n",
        "fastapi_thread = threading.Thread(target=run_fastapi, daemon=True)\n",
        "fastapi_thread.start()\n",
        "# Give it a few seconds to start up\n",
        "time.sleep(5)\n",
        "print(\"FastAPI setup complete. Public URL should be printed above.\")\n",
        "# Note: The server will run until the Colab runtime is disconnected or the thread is stopped."
      ],
      "metadata": {
        "id": "wM5z5Ap5OCqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title << 5. Frontend Development with Flutter (Conceptual Outline) >>\n",
        "\n",
        "# Flutter is a UI toolkit for building natively compiled applications for mobile, web, and desktop from a single codebase.\n",
        "# We cannot run Flutter code directly in Colab, but we can outline the key components and interactions.\n",
        "\n",
        "\"\"\"\n",
        "--- Flutter Frontend Overview ---\n",
        "\n",
        "1.  **Project Setup:**\n",
        "    * Create a new Flutter project: `flutter create hospital_stress_dashboard`\n",
        "    * Add dependencies to `pubspec.yaml`:\n",
        "        * `http` or `dio`: For making REST API calls to the FastAPI backend.\n",
        "        * `provider` or `flutter_bloc`: For state management.\n",
        "        * `syncfusion_flutter_charts` or `fl_chart`: For displaying dashboards and graphs.\n",
        "        * `web_socket_channel`: For real-time alerts (if using WebSockets).\n",
        "        * `intl`: For date/time formatting.\n",
        "        * `shared_preferences`: For storing basic user settings or tokens.\n",
        "\n",
        "2.  **Core Widgets/Screens:**\n",
        "    * `LoginScreen`: Handles user authentication (e.g., using email/password, OAuth against the backend).\n",
        "    * `DashboardScreen`: Main screen showing:\n",
        "        * Overall stress level overview (average, trends).\n",
        "        * List of doctors with current stress scores (color-coded).\n",
        "        * Upcoming schedule view (e.g., weekly calendar).\n",
        "        * Alerts section.\n",
        "    * `DoctorDetailScreen`: Shows detailed stress history (graph), recent logs, current schedule for a specific doctor.\n",
        "    * `ScheduleScreen`: Displays the full schedule, possibly with filtering options (by date, doctor, shift). Allows admins to trigger manual optimization.\n",
        "    * `SettingsScreen`: App settings, notification preferences.\n",
        "\n",
        "3.  **Backend Integration (Services/Repositories):**\n",
        "    * `ApiService` class:\n",
        "        * `login(email, password)` -> Returns user info/token.\n",
        "        * `getStressLevels()` -> Fetches aggregated stress data. Calls `GET /doctors` or a dedicated stats endpoint.\n",
        "        * `getDoctorStressLog(doctorId)` -> Fetches `GET /stress_logs?doctor_id=...`.\n",
        "        * `getCurrentSchedule()` -> Fetches `GET /schedules?date=...`.\n",
        "        * `triggerOptimization(date)` -> Calls `POST /optimize_schedule`.\n",
        "        * (If Edge processing is simulated/tested): `predictStress(imageBase64, doctorId)` -> Calls `POST /predict_stress`.\n",
        "    * Use `http` or `dio` package to make calls to the FastAPI backend URL (the ngrok URL during development/testing in Colab). Handle responses, errors, and authentication headers (e.g., Bearer token).\n",
        "\n",
        "4.  **State Management (`Provider` Example):**\n",
        "    * `DoctorProvider`: Manages the list of doctors and their current stress states.\n",
        "    * `ScheduleProvider`: Manages the current schedule data.\n",
        "    * `AuthProvider`: Manages user authentication state.\n",
        "    * Widgets listen to providers using `Consumer` or `context.watch` to rebuild when data changes.\n",
        "\n",
        "5.  **Real-time Alerts:**\n",
        "    * **Option 1 (WebSockets):**\n",
        "        * Backend (FastAPI) needs WebSocket endpoint (`@app.websocket(\"/ws\")`).\n",
        "        * When high stress is detected or schedule changes, backend pushes message via WebSocket.\n",
        "        * Flutter app connects using `web_socket_channel` and listens for messages, updating UI or showing notifications.\n",
        "    * **Option 2 (Polling):**\n",
        "        * Flutter app periodically calls API endpoints (e.g., `/stress_levels`, `/notifications`) every X seconds/minutes. Simpler but less efficient.\n",
        "\n",
        "6.  **UI Components:**\n",
        "    * Use `ListView.builder` to display lists of doctors/schedule entries.\n",
        "    * Use chart libraries (`syncfusion_flutter_charts`, `fl_chart`) to render stress trend graphs and schedule visualizations.\n",
        "    * Use `Card`, `ListTile`, `AppBar`, `BottomNavigationBar` for layout.\n",
        "    * Implement visual cues for stress levels (e.g., green/yellow/red indicators).\n",
        "\n",
        "7.  **Example Snippet (Conceptual API Call with `http`):**\n",
        "\n",
        "    ```dart\n",
        "    // // conceptual_api_service.dart (Flutter/Dart code)\n",
        "    // import 'package:http/http.dart' as http;\n",
        "    // import 'dart:convert';\n",
        "\n",
        "    // class ApiService {\n",
        "    //   final String _baseUrl = \"YOUR_FASTAPI_BASE_URL\"; // e.g., ngrok URL\n",
        "\n",
        "    //   Future<Map<String, dynamic>> getDoctorStress(int doctorId) async {\n",
        "    //     final response = await http.get(\n",
        "    //       Uri.parse('$_baseUrl/stress_logs?doctor_id=$doctorId&latest=true'), // Example endpoint\n",
        "    //       headers: {'Authorization': 'Bearer YOUR_AUTH_TOKEN'}, // If auth is needed\n",
        "    //     );\n",
        "\n",
        "    //     if (response.statusCode == 200) {\n",
        "    //       return jsonDecode(response.body); // Expecting JSON response\n",
        "    //     } else {\n",
        "    //       throw Exception('Failed to load stress data: ${response.statusCode}');\n",
        "    //     }\n",
        "    //   }\n",
        "\n",
        "    //    Future<Map<String, dynamic>> triggerOptimization(DateTime date) async {\n",
        "    //      final response = await http.post(\n",
        "    //        Uri.parse('$_baseUrl/optimize_schedule'),\n",
        "    //        headers: {\n",
        "    //          'Content-Type': 'application/json; charset=UTF-8',\n",
        "    //          'Authorization': 'Bearer YOUR_AUTH_TOKEN',\n",
        "    //        },\n",
        "    //        body: jsonEncode(<String, dynamic>{\n",
        "    //          'target_date': date.toIso8601String().substring(0, 10), // Format as YYYY-MM-DD\n",
        "    //          'force_run': false,\n",
        "    //        }),\n",
        "    //      );\n",
        "\n",
        "    //      if (response.statusCode == 200) {\n",
        "    //        return jsonDecode(response.body);\n",
        "    //      } else {\n",
        "    //       throw Exception('Failed to trigger optimization: ${response.statusCode} ${response.body}');\n",
        "    //      }\n",
        "    //    }\n",
        "    // }\n",
        "    ```\n",
        "\n",
        "--- End Conceptual Outline ---\n",
        "\"\"\"\n",
        "print(\"Flutter Frontend conceptual outline generated.\")\n",
        "print(\"Actual implementation requires Flutter SDK and IDE (like VS Code or Android Studio).\")"
      ],
      "metadata": {
        "id": "4w7VlKRZOKuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title << 6. Gemini 2.0 Integration >>\n",
        "\n",
        "# Ensure the google-generativeai library is installed and API key is configured.\n",
        "# Done in Setup cell.\n",
        "\n",
        "# --- Initialize Gemini Model ---\n",
        "# Use a model suitable for text generation/analysis (e.g., 'gemini-1.5-flash' or 'gemini-pro')\n",
        "# Gemini 2.0 isn't an official model name; using available generative models.\n",
        "try:\n",
        "    gemini_model = genai.GenerativeModel('gemini-1.5-flash') # Or 'gemini-pro'\n",
        "    print(\"Gemini Generative Model initialized.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing Gemini Model: {e}\")\n",
        "    print(\"Ensure API key is valid and configured.\")\n",
        "    gemini_model = None\n",
        "\n",
        "\n",
        "# --- Function to Get Insights from Stress Data using Gemini ---\n",
        "def get_stress_insights_with_gemini(doctor_id, current_stress_score, recent_stress_logs=None):\n",
        "    \"\"\"\n",
        "    Generates natural language insights about a doctor's stress using Gemini.\n",
        "\n",
        "    Args:\n",
        "        doctor_id (int): The ID of the doctor.\n",
        "        current_stress_score (float): The latest predicted stress score (0-1).\n",
        "        recent_stress_logs (list[dict], optional): List of recent logs [{'timestamp': ..., 'score': ...}].\n",
        "\n",
        "    Returns:\n",
        "        str: Natural language insights or an error message.\n",
        "    \"\"\"\n",
        "    if not gemini_model:\n",
        "        return \"Gemini model not available.\"\n",
        "\n",
        "    # --- Construct the Prompt ---\n",
        "    prompt = f\"\"\"\n",
        "    Analyze the stress level of Doctor ID {doctor_id}.\n",
        "\n",
        "    Current Situation:\n",
        "    - The doctor's latest predicted stress score is {current_stress_score:.2f} (where 0 is no stress, 1 is high stress).\n",
        "    - A score above {STRESS_THRESHOLD} is considered potentially high stress requiring attention.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if recent_stress_logs:\n",
        "        prompt += \"\\nRecent Stress Trend (last few readings):\\n\"\n",
        "        for log in recent_stress_logs[-5:]: # Show last 5 logs max\n",
        "             timestamp = log.get('timestamp', 'N/A')\n",
        "             score = log.get('raw_stress_score', 'N/A')\n",
        "             if isinstance(timestamp, datetime.datetime):\n",
        "                 timestamp_str = timestamp.strftime('%Y-%m-%d %H:%M')\n",
        "             else:\n",
        "                 timestamp_str = str(timestamp)\n",
        "             prompt += f\"- Timestamp: {timestamp_str}, Score: {score:.2f}\\n\"\n",
        "    else:\n",
        "        prompt += \"\\nNo recent stress trend data available.\\n\"\n",
        "\n",
        "    prompt += f\"\"\"\n",
        "    Based on this information:\n",
        "    1. Briefly assess the current stress level (e.g., Low, Moderate, High, Critical).\n",
        "    2. If the stress is high (above {STRESS_THRESHOLD}) or shows a concerning upward trend, suggest potential contributing factors (e.g., workload, consecutive shifts - you can infer possibilities).\n",
        "    3. Recommend one or two brief, actionable insights or considerations for the scheduling system or supervisor (e.g., 'Consider assigning shorter shifts', 'Monitor closely', 'Ensure adequate rest period before next shift', 'Seems stable').\n",
        "\n",
        "    Keep the response concise and professional for a hospital operations context. Do not give medical advice.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Call Gemini API ---\n",
        "    try:\n",
        "        print(f\"\\n--- Calling Gemini for Doctor {doctor_id} (Stress: {current_stress_score:.2f}) ---\")\n",
        "        # print(\"Prompt:\", prompt) # Uncomment to debug prompt\n",
        "\n",
        "        response = gemini_model.generate_content(prompt)\n",
        "\n",
        "        # --- Process Response ---\n",
        "        # Check for safety ratings and blocked prompts if necessary\n",
        "        # print(response.prompt_feedback)\n",
        "\n",
        "        if response.candidates and hasattr(response.candidates[0], 'content') and response.candidates[0].content.parts:\n",
        "            insight = response.text # Access text directly if available\n",
        "            print(f\"Gemini Insight for Doctor {doctor_id}:\\n{insight}\")\n",
        "            return insight\n",
        "        else:\n",
        "             # Handle cases where response is blocked or empty\n",
        "             print(f\"Gemini Warning: Received no content or response was blocked for Doctor {doctor_id}.\")\n",
        "             # Check prompt_feedback for details if available\n",
        "             feedback = getattr(response, 'prompt_feedback', None)\n",
        "             if feedback:\n",
        "                 print(f\"Prompt Feedback: {feedback}\")\n",
        "             return f\"Gemini could not generate insights for Doctor {doctor_id}. The request might have been blocked or returned no content.\"\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error calling Gemini API: {e}\")\n",
        "        return f\"Error communicating with Gemini: {e}\"\n",
        "\n",
        "# --- Example Usage (using data from previous steps) ---\n",
        "\n",
        "# Fetch some recent logs for a doctor (Example Doctor 0)\n",
        "doctor_to_analyze = 0\n",
        "fetched_logs = []\n",
        "try:\n",
        "    conn = engine.connect()\n",
        "    select_logs = db.select(stress_logs_table.c.timestamp, stress_logs_table.c.raw_stress_score)\\\n",
        "                    .where(stress_logs_table.c.doctor_id == doctor_to_analyze)\\\n",
        "                    .order_by(stress_logs_table.c.timestamp.desc())\\\n",
        "                    .limit(5)\n",
        "    results = conn.execute(select_logs)\n",
        "    # Use .mappings().all() for easy dictionary conversion if using newer SQLAlchemy versions\n",
        "    # fetched_logs = results.mappings().all() # Preferred method\n",
        "    # Manual conversion for broader compatibility:\n",
        "    fetched_logs = [{'timestamp': row[0], 'raw_stress_score': row[1]} for row in results]\n",
        "\n",
        "    conn.close()\n",
        "    print(f\"\\nFetched {len(fetched_logs)} recent logs for Doctor {doctor_to_analyze}.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error fetching recent logs for Gemini example: {e}\")\n",
        "\n",
        "\n",
        "# Get the current stress score used in optimization (or predict again if needed)\n",
        "if 'simulated_current_stress' in locals():\n",
        "    current_score_example = simulated_current_stress[doctor_to_analyze]\n",
        "elif 'current_stress_levels' in locals():\n",
        "     current_score_example = current_stress_levels[doctor_to_analyze]\n",
        "else:\n",
        "     current_score_example = 0.5 # Default example score\n",
        "\n",
        "\n",
        "# Call the Gemini function\n",
        "if gemini_model:\n",
        "  gemini_insight = get_stress_insights_with_gemini(\n",
        "      doctor_id=doctor_to_analyze,\n",
        "      current_stress_score=current_score_example,\n",
        "      recent_stress_logs=fetched_logs\n",
        "  )\n",
        "else:\n",
        "    print(\"Skipping Gemini insight generation as model is not initialized.\")\n",
        "\n",
        "\n",
        "# --- Conversational Assistant (Conceptual) ---\n",
        "# Gemini can power a chatbot for hospital staff:\n",
        "# - Staff could ask: \"Show Dr. Smith's stress trend.\" -> App queries DB, formats data, sends to Gemini for summarization.\n",
        "# - Staff could ask: \"What are the constraints for scheduling night shifts?\" -> Gemini accesses configured rules/knowledge base.\n",
        "# - Staff could ask: \"Suggest a replacement for Dr. Jones' shift tomorrow afternoon.\" -> Gemini interacts with the optimization engine/DB to find suitable candidates.\n",
        "\n",
        "def ask_gemini_assistant(query, context_data=None):\n",
        "    \"\"\"Simulates asking a question to a Gemini-powered assistant.\"\"\"\n",
        "    if not gemini_model:\n",
        "        return \"Gemini assistant not available.\"\n",
        "\n",
        "    prompt = f\"You are a helpful assistant for hospital staff using a stress detection and scheduling system.\\n\"\n",
        "    prompt += f\"User Query: '{query}'\\n\\n\"\n",
        "    if context_data:\n",
        "         prompt += f\"Relevant Context Data:\\n{json.dumps(context_data, indent=2, default=str)}\\n\\n\" # Provide context if needed\n",
        "\n",
        "    prompt += \"Provide a helpful and concise answer based on the query and context.\"\n",
        "\n",
        "    try:\n",
        "        print(f\"\\n--- Calling Gemini Assistant ---\")\n",
        "        response = gemini_model.generate_content(prompt)\n",
        "        if response.candidates and hasattr(response.candidates[0], 'content') and response.candidates[0].content.parts:\n",
        "            answer = response.text\n",
        "            print(f\"Gemini Assistant Response:\\n{answer}\")\n",
        "            return answer\n",
        "        else:\n",
        "            print(\"Gemini Assistant Warning: No content or blocked response.\")\n",
        "            return \"Assistant could not process the query.\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error calling Gemini Assistant API: {e}\")\n",
        "        return f\"Error communicating with Gemini: {e}\"\n",
        "\n",
        "# Example Assistant Query\n",
        "assistant_query = \"Summarize the current schedule status for Day 1.\"\n",
        "# In a real app, fetch schedule data for Day 1 as context\n",
        "schedule_context = {\"Day1_Schedule\": \"Dr. 0 (S0), Dr. 1 (S1), Dr. 2 (S2), Dr. 3 (S0), Dr. 4 (S1)\"} # Example\n",
        "if gemini_model:\n",
        "  assistant_response = ask_gemini_assistant(assistant_query, schedule_context)\n",
        "else:\n",
        "  print(\"Skipping Gemini assistant query as model is not initialized.\")"
      ],
      "metadata": {
        "id": "GQyz7y8FON3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title << 7. Privacy, Security & Deployment >>\n",
        "\n",
        "# --- 7.1 Privacy & Consent ---\n",
        "print(\"\\n--- Privacy Considerations ---\")\n",
        "print(\"1.  **Explicit Consent:** Absolutely crucial. Before monitoring any facial expressions:\")\n",
        "print(\"    - Obtain clear, informed, written consent from each doctor.\")\n",
        "print(\"    - Explain exactly WHAT data is collected (images, features, stress scores), HOW it's used (scheduling, anonymized reporting), WHO can access it, and HOW LONG it's stored.\")\n",
        "print(\"    - Ensure consent is specific, granular (if possible), and easily revocable.\")\n",
        "print(\"2.  **Anonymization/De-identification:**\")\n",
        "print(\"    - **Edge Processing:** Ideally, perform face detection and feature extraction on the edge device. Only send numerical features or anonymized/cropped face images (if strictly necessary) to the backend.\")\n",
        "print(\"    - **Data Minimization:** Collect only the minimum data needed. Do you need full video, or just snapshots? Do you need high-res images, or can lower-res suffice for feature extraction?\")\n",
        "print(\"    - **Feature Focus:** Store landmarks, stress scores, and metadata. AVOID storing raw facial images long-term unless essential for regulatory/audit reasons (with strict access controls).\")\n",
        "print(\"    - **Aggregation:** Report on team/department stress levels in aggregate to avoid singling out individuals in general reports.\")\n",
        "print(\"3.  **Data Security (see below):** Protect the data collected.\")\n",
        "print(\"4.  **Purpose Limitation:** Use the data ONLY for the consented purposes (stress detection for scheduling optimization and well-being support). Do not repurpose for performance evaluation or disciplinary actions unless explicitly consented to and ethically reviewed.\")\n",
        "print(\"5.  **Transparency:** Allow doctors to view their own stress data and understand how it influences their schedule.\")\n",
        "print(\"6.  **Compliance:** Adhere to relevant regulations (e.g., GDPR in Europe, HIPAA in the US). Consult legal/privacy experts.\")\n",
        "print(\"7.  **Ethical Review:** Consider review by an institutional ethics committee due to the sensitive nature of biometric data and potential impact on staff.\")\n",
        "\n",
        "# --- 7.2 Security Measures ---\n",
        "print(\"\\n--- Security Measures ---\")\n",
        "print(\"1.  **Authentication & Authorization:**\")\n",
        "print(\"    - Secure API endpoints (FastAPI): Use OAuth2 (e.g., with JWT Bearer tokens) or API Keys for server-to-server communication.\")\n",
        "print(\"    - Implement role-based access control (RBAC). Admins can trigger optimization, view all data; doctors can view own data; system components have specific permissions.\")\n",
        "print(\"    - Secure Frontend Login: Use robust password hashing (e.g., bcrypt), consider Multi-Factor Authentication (MFA).\")\n",
        "print(\"2.  **Data Encryption:**\")\n",
        "print(\"    - **In Transit:** Use HTTPS (TLS/SSL) for all API communication (ngrok provides this for Colab tunnels; use load balancers/reverse proxies like Nginx/Traefik in production).\")\n",
        "print(\"    - **At Rest:** Encrypt sensitive data in the database (SQL Server TDE, application-level encryption for specific fields). Encrypt backups.\")\n",
        "print(\"3.  **Input Validation:** Sanitize and validate all inputs to API endpoints (Pydantic helps in FastAPI) to prevent injection attacks.\")\n",
        "print(\"4.  **Secrets Management:** DO NOT hardcode API keys, database credentials, or secret keys. Use environment variables, Docker secrets, or dedicated secrets managers (GCP Secret Manager, AWS Secrets Manager, HashiCorp Vault).\")\n",
        "print(\"5.  **Secure Dependencies:** Regularly scan dependencies for vulnerabilities (e.g., `pip-audit`, `safety`, GitHub Dependabot/Snyk).\")\n",
        "print(\"6.  **Rate Limiting:** Protect API endpoints from abuse by implementing rate limiting.\")\n",
        "print(\"7.  **Logging & Monitoring:** Log security events (login attempts, access violations, errors). Monitor system health and performance.\")\n",
        "print(\"8.  **Container Security:** Scan Docker images for vulnerabilities. Run containers with least privilege.\")\n",
        "\n",
        "# --- 7.3 Deployment Strategy ---\n",
        "print(\"\\n--- Deployment Strategy ---\")\n",
        "print(\"1.  **Containerization (Docker):**\")\n",
        "print(\"    - Create `Dockerfile` for the FastAPI backend (including ONNX runtime, OR-Tools, etc.).\")\n",
        "# Example Dockerfile structure (conceptual):\n",
        "\"\"\"\n",
        "# Dockerfile Example (Conceptual)\n",
        "# Use an appropriate base image (e.g., Python slim with build tools)\n",
        "# FROM python:3.10-slim\n",
        "\n",
        "# WORKDIR /app\n",
        "\n",
        "# Install system dependencies (like build-essential for some Python packages, ODBC drivers for SQL Server)\n",
        "# RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
        "#    build-essential \\\n",
        "#    unixodbc-dev # Example for pyodbc\n",
        "    # Add commands to install Microsoft ODBC Driver if needed\n",
        "#    && rm -rf /var/lib/apt/lists/*\n",
        "\n",
        "# Copy requirements and install Python packages\n",
        "# COPY requirements.txt .\n",
        "# RUN pip install --no-cache-dir -r requirements.txt\n",
        "\n",
        "# Copy application code, models, etc.\n",
        "# COPY . .\n",
        "# Make sure ONNX models are copied into the image or loaded from a volume\n",
        "\n",
        "# Expose the port FastAPI runs on\n",
        "# EXPOSE 8000\n",
        "\n",
        "# Command to run the application\n",
        "# CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"] # Assuming your FastAPI app object is in main.py\n",
        "\"\"\"\n",
        "print(\"    - Create `Dockerfile` for any other services (e.g., time-series aggregator if separate).\")\n",
        "print(\"    - Use Docker Compose for local development/testing setup.\")\n",
        "\n",
        "print(\"2.  **Cloud Platform (GCP or AWS):**\")\n",
        "print(\"    - **Compute:**\")\n",
        "print(\"        - **GCP:** Google Kubernetes Engine (GKE) for orchestration, Cloud Run for serverless containers (good for stateless APIs like inference if cold starts are acceptable), Vertex AI Endpoints for managed ML model serving (supports ONNX, TF, PyTorch, custom containers, includes scaling, monitoring).\")\n",
        "print(\"        - **AWS:** Elastic Kubernetes Service (EKS), Elastic Container Service (ECS), Fargate (serverless containers), SageMaker Endpoints (managed ML serving).\")\n",
        "print(\"    - **Database:**\")\n",
        "print(\"        - **GCP:** Cloud SQL (managed PostgreSQL, MySQL, SQL Server), AlloyDB.\")\n",
        "print(\"        - **AWS:** RDS (managed instances), Aurora.\")\n",
        "print(\"    - **API Gateway:**\")\n",
        "print(\"        - **GCP:** API Gateway or Cloud Load Balancer + IAP (Identity-Aware Proxy).\")\n",
        "print(\"        - **AWS:** API Gateway.\")\n",
        "print(\"    - **Monitoring & Logging:**\")\n",
        "print(\"        - **GCP:** Cloud Monitoring, Cloud Logging.\")\n",
        "print(\"        - **AWS:** CloudWatch.\")\n",
        "print(\"    - **Secrets Management:** GCP Secret Manager, AWS Secrets Manager.\")\n",
        "\n",
        "print(\"3.  **CI/CD Pipeline:**\")\n",
        "print(\"    - Use tools like GitHub Actions, GitLab CI, Jenkins, Google Cloud Build, AWS CodePipeline.\")\n",
        "print(\"    - Automate testing (unit, integration, API tests), security scanning, building Docker images, and deploying to staging/production environments.\")\n",
        "\n",
        "print(\"4.  **Model Deployment & MLOps:**\")\n",
        "print(\"    - **Model Registry:** Use MLflow, Vertex AI Model Registry, or SageMaker Model Registry to version and track models.\")\n",
        "print(\"    - **Inference Optimization:** Use ONNX Runtime, TensorRT (on NVIDIA GPUs), TensorFlow Lite (for edge/mobile if needed).\")\n",
        "print(\"    - **Monitoring:** Track model performance (accuracy, F1, AUC), prediction drift, data drift, inference latency, resource utilization.\")\n",
        "print(\"    - **Retraining Strategy:** Define triggers for retraining (e.g., performance degradation, new data available) and automate the retraining pipeline.\")\n",
        "\n",
        "print(\"5.  **Edge Deployment (Optional):**\")\n",
        "print(\"    - If face detection/feature extraction runs on-site: Use smaller models (MobileNet, EfficientNet-Lite), TensorFlow Lite, ONNX Runtime for Edge.\")\n",
        "print(\"    - Manage edge devices using IoT platforms (GCP IoT Core - *being deprecated, consider alternatives*, AWS IoT Core).\")\n",
        "\n",
        "\n",
        "# --- 7.4 Dockerfile Example Snippet ---\n",
        "dockerfile_content = \"\"\"\n",
        "# Use an official Python runtime as a parent image\n",
        "FROM python:3.10-slim\n",
        "\n",
        "# Set environment variables (optional but good practice)\n",
        "ENV PYTHONDONTWRITEBYTECODE 1\n",
        "ENV PYTHONUNBUFFERED 1\n",
        "\n",
        "# Set the working directory in the container\n",
        "WORKDIR /app\n",
        "\n",
        "# Install system dependencies if needed (e.g., for opencv, onnxruntime, pyodbc)\n",
        "# Example: RUN apt-get update && apt-get install -y --no-install-recommends libgl1-mesa-glx libglib2.0-0 && rm -rf /var/lib/apt/lists/*\n",
        "# Example for SQL Server ODBC: RUN apt-get update && apt-get install -y curl apt-transport-https gnupg2 unixodbc-dev ... && #<commands to install msodbcsql17>\n",
        "\n",
        "# Install pip requirements\n",
        "# Copy only requirements first to leverage Docker cache\n",
        "COPY requirements.txt .\n",
        "RUN pip install --no-cache-dir -r requirements.txt\n",
        "\n",
        "# Copy the rest of the application code\n",
        "COPY . .\n",
        "\n",
        "# Ensure ONNX model is copied or accessible (e.g., via volume mount)\n",
        "# COPY ./onnx_models /app/onnx_models\n",
        "\n",
        "# Expose the port the app runs on\n",
        "EXPOSE 8000\n",
        "\n",
        "# Define the command to run the application\n",
        "# Assumes your FastAPI app instance is named 'app' in a file named 'main.py'\n",
        "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
        "\"\"\"\n",
        "print(\"\\n--- Example Dockerfile Structure ---\")\n",
        "print(\"```dockerfile\")\n",
        "print(dockerfile_content)\n",
        "print(\"```\")\n",
        "print(\"Note: Adapt system dependencies and entrypoint based on the final 'main.py' structure.\")\n"
      ],
      "metadata": {
        "id": "bmr4CBSjOUG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title << 8. Conclusion & Next Steps >>\n",
        "\n",
        "print(\"\\n--- Project Summary ---\")\n",
        "print(\"This Colab notebook provides a comprehensive blueprint and implementation for an AI-driven platform to:\")\n",
        "print(\"1.  **Detect Doctor Stress:** Using facial expression analysis with Deep Learning models (ResNet, EfficientNet) trained on relevant datasets (like FER2013 mapped to stress) and exported to ONNX for efficient inference.\")\n",
        "print(\"2.  **Optimize Shifts:** Employing Google OR-Tools (CP-SAT solver) to generate fair and constraint-compliant schedules that consider predicted stress levels, skills, availability, and rules.\")\n",
        "print(\"3.  **Integrate Systems:** Outlining a microservices architecture using FastAPI for the backend, defining a SQL Server database schema, conceptualizing a Flutter frontend, and integrating the Gemini API for enhanced insights and assistant capabilities.\")\n",
        "print(\"4.  **Address MLOps & Productionization:** Covering essential aspects like data preprocessing, augmentation, model evaluation, hyperparameter tuning concepts, deployment strategies (Docker, Cloud), security, and privacy.\")\n",
        "\n",
        "print(\"\\n--- Key Components Implemented/Demonstrated ---\")\n",
        "print(\"- Kaggle API for dataset download.\")\n",
        "print(\"- Data preprocessing pipeline (face detection placeholder, normalization, resizing).\")\n",
        "print(\"- Training and evaluation of two CNN models (ResNet, EfficientNet) for stress classification (binary).\")\n",
        "print(\"- Model export to ONNX format.\")\n",
        "print(\"- Shift optimization using OR-Tools CP-SAT with various constraints (skills, workload, stress).\")\n",
        "print(\"- Conceptual database schema using SQLAlchemy.\")\n",
        "print(\"- FastAPI backend skeleton with prediction and optimization endpoints.\")\n",
        "print(\"- Loading and using the ONNX model within FastAPI.\")\n",
        "print(\"- Integration with the Gemini API for generating stress insights and conversational support.\")\n",
        "print(\"- Discussion of privacy, security, deployment, and MLOps best practices.\")\n",
        "print(\"- Conceptual outline for a Flutter frontend.\")\n",
        "\n",
        "print(\"\\n--- Limitations & Future Work ---\")\n",
        "print(\"- **Stress Dataset:** Relied heavily on FER2013 mapped to stress. A dedicated, validated facial *stress* dataset would significantly improve model accuracy and reliability.\")\n",
        "print(\"- **rPPG:** Not implemented; incorporating physiological signals like heart rate variability via rPPG could enhance stress detection.\")\n",
        "print(\"- **Temporal Analysis:** Models trained on single images. Using LSTMs/GRUs/Transformers on sequences of features over time would capture temporal dynamics of stress.\")\n",
        "print(\"- **Hyperparameter Optimization:** Only conceptualized; full Optuna/Ray Tune integration needed for optimal model performance.\")\n",
        "print(\"- **TensorRT:** Only conceptualized; requires specific hardware and environment setup.\")\n",
        "print(\"- **RL for Scheduling:** Only conceptualized; requires building a complex simulation environment.\")\n",
        "print(\"- **Robust Constraints:** Consecutive shift and minimum rest constraints in OR-Tools need more robust implementation.\")\n",
        "print(\"- **Error Handling & Resilience:** Production code needs more comprehensive error handling, retries, and monitoring.\")\n",
        "print(\"- **Real-time Processing:** Handling real-time video streams efficiently requires optimization (e.g., frame skipping, batching).\")\n",
        "print(\"- **Full Frontend/Backend:** Only backend API skeleton and Flutter concepts provided. Full implementation required.\")\n",
        "print(\"- **Ethical & Bias Audit:** Thoroughly audit models and system for potential biases (e.g., performance differences across demographics) and ethical implications.\")\n",
        "print(\"- **User Feedback Loop:** Incorporate feedback from doctors and administrators to iteratively improve the system.\")\n",
        "\n",
        "print(\"\\n--- Final Thoughts ---\")\n",
        "print(\"Building such a system requires careful consideration of technical, ethical, and practical aspects. This notebook serves as a strong starting point, demonstrating the feasibility and integration of various AI and software engineering components. Continuous iteration, validation with real-world data and users, and a strong focus on privacy and ethics are paramount for success in a sensitive domain like healthcare.\")"
      ],
      "metadata": {
        "id": "QjpZ0p2TOXds"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}